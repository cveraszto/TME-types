{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95314671-9bb7-47b3-8236-26717f833950",
   "metadata": {},
   "source": [
    "This notebook is not needed anymore. It was used to estimate additional scaling parameters for INH EXC and GLIAL cells to match cell densities to Sebastien's validation script. \n",
    "\n",
    "The resulting parameters are then applied in the 3rd part of the density1.5_multi_scaling step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f6735e-a824-47db-b070-11949078f255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/ssd/apps/bsd/2024-02-01/stage_externals/install_gcc-12.3.0-skylake/python-3.11.6-bj4i6m/bin/python\n",
      "2.2.1\n",
      "3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0]\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/site-packages/pandas/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#Newer pickle versions are not compatible\n",
    "!which python\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "import sys\n",
    "print(sys.version)\n",
    "import inspect\n",
    "print(inspect.getfile(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ed2ff2-1d06-41b2-99f5-8e12233ed1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nrrd\n",
    "import multiprocessing as mp\n",
    "import re, os, copy\n",
    "from voxcell import RegionMap\n",
    "import ast\n",
    "\n",
    "import sys\n",
    "sys.path.append('/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/notebooks/scripts/')\n",
    "\n",
    "from helper_functions import get_all_filenames, get_csv_filenames, extract_prefix_from_filenames, read_and_concat_csv_files_new, combine_rows_and_calculate_average, create_combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800cc217-9414-44d5-bcf6-f273f7faee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_base = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/'\n",
    "root_folder = f\"{download_base}results/density_calculations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a870c8-eed5-4c79-b632-15bedc655d10",
   "metadata": {},
   "source": [
    "# Load estimated non-scaled densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f93fb8d-12fa-4449-a6ff-867918e40c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of memberships at all levels: 3489\n",
      "number of memberships at substructure level: 737\n",
      "CPU times: user 37.1 s, sys: 142 ms, total: 37.3 s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Get all regional density data\n",
    "folder_path = f\"{root_folder}csv/\"\n",
    "filenames = get_all_filenames(folder_path)\n",
    "csv_filenames = get_csv_filenames(folder_path)\n",
    "prefixes = extract_prefix_from_filenames(csv_filenames)\n",
    "unique_prefixes = sorted(list(set(prefixes)))\n",
    "\n",
    "#Create a dict of df, each containing a cell type's occurence in all regions and its densities in all regions\n",
    "result_dataframes = read_and_concat_csv_files_new(csv_filenames, unique_prefixes, folder_path)\n",
    "combined_result_dataframes = combine_rows_and_calculate_average(result_dataframes)\n",
    "shuffled_combined_dataframes = create_combined_dataframe(combined_result_dataframes)\n",
    "\n",
    "file = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/metadata/parcellation_to_parcellation_term_membership_extend.csv'\n",
    "parcellation_annotation = pd.read_csv(file)\n",
    "print(\"number of memberships at all levels:\",len(parcellation_annotation))\n",
    "parcellation_annotation = parcellation_annotation[parcellation_annotation['parcellation_term_set_name'] == 'substructure'] \n",
    "print(\"number of memberships at substructure level:\",len(parcellation_annotation))\n",
    "volumes = parcellation_annotation[['parcellation_label', 'parcellation_index', 'parcellation_term_acronym', 'parcellation_term_set_name', 'parcellation_term_name', 'voxel_count', 'volume_mm3', 'label_numbers', 'cluster_as_filename']]\n",
    "substructure_vol = volumes[volumes['parcellation_term_set_name'] == 'substructure']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bd77de-18a2-42bd-9dec-c2faa892c021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5274"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffled_combined_dataframes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dad2681f-19bc-4cfb-99c2-9766157850bc",
   "metadata": {},
   "source": [
    "# Initialize a list to store keys with problematic values\n",
    "keys_with_issues = []\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for key, df in combined_result_dataframes.items():\n",
    "    # Check for NaN values\n",
    "    has_nan = df.isna().values.any()\n",
    "    \n",
    "    # Check for inf values\n",
    "    has_inf = df.isin([np.inf, -np.inf]).values.any()\n",
    "    \n",
    "    # Check for negative values\n",
    "    has_negative = (df < 0).values.any()\n",
    "    \n",
    "    # If any issues are found, append the key to the list\n",
    "    if has_nan or has_inf or has_negative:\n",
    "        keys_with_issues.append({\n",
    "            \"key\": key,\n",
    "            \"has_nan\": has_nan,\n",
    "            \"has_inf\": has_inf,\n",
    "            \"has_negative\": has_negative\n",
    "        })\n",
    "\n",
    "# Display the keys that contain issues\n",
    "if keys_with_issues:\n",
    "    print(\"The following DataFrames have issues:\")\n",
    "    for issue in keys_with_issues:\n",
    "        print(f\"Key: {issue['key']}, NaN: {issue['has_nan']}, Inf: {issue['has_inf']}, Negative: {issue['has_negative']}\")\n",
    "else:\n",
    "    print(\"No issues found in combined_result_dataframes DataFrames.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5504478-bf41-40ad-9fd6-c8f1f2b295f9",
   "metadata": {},
   "source": [
    "#write PICKLE file with densities\n",
    "root_folder = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/'\n",
    "# Assuming 'total_cells' is your dictionary of DataFrames\n",
    "with open(f'{root_folder}non_scaled_densities_new.pickle', 'wb') as f:\n",
    "    pickle.dump(combined_result_dataframes, f)\n",
    "\n",
    "print(f\"Pickle file saved here: {root_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bc7bc1-9e5a-4c7a-b239-ef5f91138fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CCFv3 annotation volumes (choose 1) for calculating total cell counts\n",
    "# data_folder = \"/gpfs/bbp.cscs.ch/project/proj84/piluso/share/general/warped_augmented_CCFv3/\"\n",
    "# CCFv3, _ = nrrd.read(f'{data_folder}annotation_25_2022_CCFv3.nrrd')\n",
    "# data_folder = \"/gpfs/bbp.cscs.ch/data/project/proj62/csaba/atlas/bbp_prod_files/2022/\"\n",
    "# CCFv3, _ = nrrd.read(f'{data_folder}annotation_25.nrrd')\n",
    "data_folder = \"/gpfs/bbp.cscs.ch/project/proj84/piluso/share/general/warped_augmented_CCFv3/\"\n",
    "CCFv3, _ = nrrd.read(f'{data_folder}annotation_25_2022_CCFv3a.nrrd')\n",
    "\n",
    "# Create float copy of the annotation voluem, update values in CCFv3 based on conditions\n",
    "#CCFv3_copy = np.copy(CCFv3).astype('float64')\n",
    "\n",
    "#hierarchy_json = '/gpfs/bbp.cscs.ch/home/veraszto/bbp_prod_files/1.json'\n",
    "hierarchy_json = '/gpfs/bbp.cscs.ch/data/project/proj84/atlas_pipeline_runs/2024-05-15T22:44:26+02:00/hierarchy_ccfv3_l23split_barrelsplit.json'\n",
    "region_map = RegionMap.load_json(hierarchy_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406c606-9785-44b0-9172-8022af577f02",
   "metadata": {},
   "source": [
    "# Local (area by area) scaling when necessary\n",
    "Scaling parameters are provided by Blue Brain in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d531c8-2e89-4c4f-9451-9c82a0b663d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the expected data:\n",
    "data = {\n",
    "    \"id\": [391, 399, 407, 415, 431, 438, 446, 454, 471, 486, 495, 504, 718, 733, 1020],\n",
    "    \"acronym\": [\n",
    "        \"CA1slm\", \"CA1so\", \"CA1sp\", \"CA1sr\", \"CA2slm\", \"CA2so\", \n",
    "        \"CA2sp\", \"CA2sr\", \"CA3slm\", \"CA3so\", \"CA3sp\", \"CA3sr\", \"VPL\", \"VPM\", \"PO\"\n",
    "    ],\n",
    "    \"Region\": [\n",
    "        \"Field CA1, stratum lacunosum-moleculare\", \"Field CA1, stratum oriens\",\n",
    "        \"Field CA1, stratum pyramidale\", \"Field CA1, stratum radiatum\",\n",
    "        \"Field CA2, stratum lacunosum-moleculare\", \"Field CA2, stratum oriens\",\n",
    "        \"Field CA2, stratum pyramidale\", \"Field CA2, stratum radiatum\",\n",
    "        \"Field CA3, stratum lacunosum-moleculare\", \"Field CA3, stratum oriens\",\n",
    "        \"Field CA3, stratum pyramidale\", \"Field CA3, stratum radiatum\",\n",
    "        \"Ventral posterolateral nucleus of the thalamus\", \"Ventral posteromedial nucleus of the thalamus\",\n",
    "        \"Posterior complex of the thalamus\"\n",
    "    ],\n",
    "    \"Expected Inhibitory Dorsal\": [\n",
    "        8420, 6480, 13520, 6480, 8420, 6480, 13520, 3240, 5780, 3170, 7560, 7890, \"\", \"\", \"\"\n",
    "    ],\n",
    "    \"Expected Inhibitory Ventral\": [\n",
    "        8820, 10500, 10130, 10500, 8820, 10500, 10130, 4260, 7200, 5460, 10540, 6590, \"\", \"\", \"\"\n",
    "    ],\n",
    "    \"Expected Neurons Dorsal\": [\n",
    "        \"\", \"\", 447500, \"\", \"\", \"\", 447500, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "    ],\n",
    "    \"Expected Neurons Ventral\": [\n",
    "        \"\", \"\", 180500, \"\", \"\", \"\", 180500, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "    ],\n",
    "    \"exc_mm3\": [\n",
    "        \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 55340.64, 67940.58, \"\"\n",
    "    ],\n",
    "    \"inh_mm3\": [\n",
    "        8620, 8490, 11825, 8490, 8620, 8490, 11825, 3750, 6490, 4315, 9050, 7240, 2126.28, 2978.61, 161.08663\n",
    "    ],\n",
    "    \"neurons_mm3\": [\n",
    "        \"\", \"\", 314000, \"\", \"\", \"\", 314000, \"\", \"\", \"\", 172400, \"\", 57466.92, 70919.19, \"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_e = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# New data to be added\n",
    "new_data = {\n",
    "    \"id\": [212, 220, 228, 236, 244, 477],\n",
    "    \"acronym\": [\n",
    "        \"MOBglomerularlayer\", \"MOBgr\", \"MOBipl\", \"MOBmi\", \"MOBopl\", \"STR\"\n",
    "    ],\n",
    "    \"Region\": [\n",
    "        \"Main olfactory bulb, glomerular layer\", \"Main olfactory bulb, granule layer\",\n",
    "        \"Main olfactory bulb, inner plexiform layer\", \"Main olfactory bulb, mitral layer\",\n",
    "        \"Main olfactory bulb, outer plexiform layer\", \"Striatum\"\n",
    "    ],\n",
    "    \"Expected Inhibitory Dorsal\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"Expected Inhibitory Ventral\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"Expected Neurons Dorsal\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"Expected Neurons Ventral\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"exc_mm3\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"inh_mm3\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"neurons_mm3\": [\n",
    "        630000, 710000, 150000, 350000, 80000, 78560\n",
    "    ]\n",
    "}\n",
    "df_new = pd.DataFrame(new_data)\n",
    "\n",
    "# Concatenate the new data with the existing data\n",
    "df_expected = pd.concat([df_e, df_new], ignore_index=True)\n",
    "df_expected.to_csv('/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/expected_data.csv', index=False)\n",
    "\n",
    "#Add children to the regions which need to change to include non-leaf-region changes:\n",
    "file = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/df_hierarchy_ccfv3_l23split_barrelsplit.csv'\n",
    "hierarchy = pd.read_csv(file, index_col=0)\n",
    "\n",
    "# Merge df_expected with hierarchy on 'id' to get the 'children' column\n",
    "df_expected = df_expected.merge(hierarchy[['id', 'children', 'acr_list']], on='id', how='left')\n",
    "\n",
    "import ast\n",
    "df_expected.loc[: , 'children'] = df_expected.loc[: , 'children'].apply(ast.literal_eval)\n",
    "\n",
    "# Add new columns to df_expected\n",
    "df_expected['exc_mm3_ratio'] = np.nan\n",
    "df_expected['inh_mm3_ratio'] = np.nan\n",
    "df_expected['neurons_mm3_ratio'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c9426-8bd0-4f50-a7e9-a54588d8b0da",
   "metadata": {},
   "source": [
    "## Total nrrd creation\n",
    "We can separate cell types into larger (hierarchical groups) and convert densities to total cell counts. We can save the results as nrrd files. \n",
    "Step 1 is to create the hierarchical cell type groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbba2e2e-1489-4e46-8229-85ccef42797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 379 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "meta_path = \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/metadata/WMB-10X/20231215/views/cell_metadata_with_cluster_annotation.csv\"\n",
    "columns_to_read = ['class', 'subclass', 'cluster']\n",
    "#metadata = pd.read_csv(meta_path, dtype={'cell_label':str}, low_memory=False)\n",
    "metadata = pd.read_csv(meta_path, usecols=columns_to_read, )\n",
    "\n",
    "n_classes = ['01 IT-ET Glut', '02 NP-CT-L6b Glut', '03 OB-CR Glut',\n",
    "       '04 DG-IMN Glut', '05 OB-IMN GABA', '06 CTX-CGE GABA',\n",
    "       '07 CTX-MGE GABA', '08 CNU-MGE GABA', '09 CNU-LGE GABA',\n",
    "       '10 LSX GABA', '11 CNU-HYa GABA', '12 HY GABA', '13 CNU-HYa Glut',\n",
    "       '14 HY Glut', '15 HY Gnrh1 Glut', '16 HY MM Glut', '17 MH-LH Glut',\n",
    "       '18 TH Glut', '19 MB Glut', '20 MB GABA', '21 MB Dopa',\n",
    "       '22 MB-HB Sero', '23 P Glut', '24 MY Glut', '25 Pineal Glut',\n",
    "       '26 P GABA', '27 MY GABA', '28 CB GABA', '29 CB Glut',]\n",
    "\n",
    "nn_classes = ['30 Astro-Epen', '31 OPC-Oligo', '32 OEC', '33 Vascular',\n",
    "       '34 Immune']\n",
    "\n",
    "exc = ['01 IT-ET Glut', '02 NP-CT-L6b Glut', '03 OB-CR Glut',\n",
    "      '04 DG-IMN Glut', '13 CNU-HYa Glut', '14 HY Glut', '15 HY Gnrh1 Glut', '16 HY MM Glut', '17 MH-LH Glut',\n",
    "      '18 TH Glut', '19 MB Glut', '23 P Glut', '24 MY Glut', '25 Pineal Glut', '29 CB Glut',]\n",
    "inh = ['05 OB-IMN GABA', '06 CTX-CGE GABA', '07 CTX-MGE GABA', '08 CNU-MGE GABA', '09 CNU-LGE GABA',\n",
    "      '10 LSX GABA', '11 CNU-HYa GABA', '12 HY GABA', '20 MB GABA', '26 P GABA', '27 MY GABA', '28 CB GABA', ]\n",
    "other = ['21 MB Dopa', '22 MB-HB Sero', ]\n",
    "exci_inhib_sum = exc + inh\n",
    "\n",
    "astrotypes = ['5206 Bergmann NN_1', '5207 Astro-CB NN_1', '5208 Astro-NT NN_1',\n",
    "       '5209 Astro-NT NN_1', '5210 Astro-NT NN_1', '5211 Astro-NT NN_1',\n",
    "       '5212 Astro-NT NN_1', '5213 Astro-NT NN_1', '5214 Astro-NT NN_2',\n",
    "       '5215 Astro-NT NN_2', '5216 Astro-NT NN_2', '5217 Astro-NT NN_2',\n",
    "       '5218 Astro-TE NN_1', '5219 Astro-TE NN_1', '5220 Astro-TE NN_1',\n",
    "       '5221 Astro-TE NN_1', '5222 Astro-TE NN_2', '5223 Astro-TE NN_2',\n",
    "       '5224 Astro-TE NN_3', '5225 Astro-TE NN_3', '5226 Astro-TE NN_3',\n",
    "       '5227 Astro-TE NN_3', '5228 Astro-TE NN_4', '5229 Astro-TE NN_5',\n",
    "       '5230 Astro-TE NN_5', '5231 Astro-OLF NN_1', '5232 Astro-OLF NN_1',\n",
    "       '5233 Astro-OLF NN_2', '5234 Astro-OLF NN_2',\n",
    "       '5235 Astro-OLF NN_3', '5236 Astro-OLF NN_3',]\n",
    "\n",
    "microglia = ['5312 Microglia NN_1']\n",
    "\n",
    "oligos = [ '5266 OPC NN_1', '5267 OPC NN_1',\n",
    "       '5268 OPC NN_1', '5269 OPC NN_1', '5270 OPC NN_1', '5271 OPC NN_2',\n",
    "       '5272 COP NN_1', '5273 COP NN_1', '5274 COP NN_1', '5275 COP NN_1',\n",
    "       '5276 COP NN_1', '5277 COP NN_1', '5278 NFOL NN_2',\n",
    "       '5279 NFOL NN_2', '5280 NFOL NN_2', '5281 NFOL NN_2',\n",
    "       '5282 MFOL NN_3', '5283 MFOL NN_3', '5284 MOL NN_4',\n",
    "       '5285 MOL NN_4', '5286 MOL NN_4', '5287 MOL NN_4', '5288 MOL NN_4',]\n",
    "\n",
    "glia = astrotypes + microglia + oligos\n",
    "\n",
    "neurontypes = np.unique(metadata[metadata['class'].isin(n_classes)]['cluster'].values)\n",
    "nonneurontypes = np.unique(metadata[metadata['class'].isin(nn_classes)]['cluster'].values)\n",
    "exctypes = np.unique(metadata[metadata['class'].isin(exc)]['cluster'].values)\n",
    "inhtypes = np.unique(metadata[metadata['class'].isin(inh)]['cluster'].values)\n",
    "othertypes = np.unique(metadata[metadata['class'].isin(other)]['cluster'].values)\n",
    "exci_inhib_sum = np.unique(metadata[metadata['class'].isin(exci_inhib_sum)]['cluster'].values)\n",
    "celltypes = np.unique(metadata['cluster'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03c00f-e2cc-4f4d-b353-5184734296a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4265aea7-d183-4645-8686-75b5a4e95778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to create total cell count values for a 3D brain\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def nrrd_for_validation(df, parcellation_annotation, CCFv3):\n",
    "    all_ids_for_df = []\n",
    "    df_comb = pd.DataFrame()\n",
    "\n",
    "    for regionname in df.index.values[0:]:\n",
    "        density = df.loc[regionname, 'density_mm3']\n",
    "        #annotation_id_info = substructures[substructures['cluster_as_filename'] == regionname]\n",
    "        annotation_id_info = parcellation_annotation[parcellation_annotation['cluster_as_filename'] == regionname]\n",
    "\n",
    "        Annotation2020ids = [int(re.search(r'\\d+$', s).group()) for s in annotation_id_info['parcellation_label'].values]\n",
    "        df_sub = pd.DataFrame({'density': density}, index=Annotation2020ids)\n",
    "        df_comb = pd.concat([df_comb, df_sub])\n",
    "        all_ids_for_df.append(Annotation2020ids)\n",
    "\n",
    "    all_ids_for_df = [value for sublist in all_ids_for_df for value in sublist]\n",
    "    all_ids_for_df.append(0)\n",
    "    #Place to put extra regions not part of Allen's Parcellation annotation\n",
    "\n",
    "    outside = 0\n",
    "    outsideid = [0]\n",
    "    df_sub = pd.DataFrame({'density': outside}, index=outsideid)\n",
    "    df_comb = pd.concat([df_comb, df_sub])\n",
    "\n",
    "    CCFv3_copy = CCFv3.copy()\n",
    "\n",
    "    # Expression is 0 in those regions where we don't have any info:\n",
    "    CCFv3_copy[~np.isin(CCFv3_copy, all_ids_for_df)] = 0.0 \n",
    "\n",
    "    # Expression is non-zero in these leaf region(s)\n",
    "    for index, row in df_comb.iterrows():\n",
    "        density_value = row['density']\n",
    "        region_id = index\n",
    "        CCFv3_copy[np.isin(CCFv3, region_id)] = density_value\n",
    "\n",
    "    #Create outside of the brain as 0\n",
    "    CCFv3_copy[np.isin(CCFv3, int(0))] = 0\n",
    "\n",
    "    return CCFv3_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64114e5d-4fbe-4fc5-9f16-02aa92d38898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_neuron_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_excitatory_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_inhibitory_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_astrotypes_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_microglia_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_oligocyte_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_glia_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_excinh_densities.nrrd\n",
      "/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/total_celltypes_densities.nrrd\n",
      "CPU times: user 56 s, sys: 4.95 s, total: 1min\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "def process_type(types, file_name):\n",
    "    # Filter DataFrames based on types\n",
    "    filtered_dataframes = {key: value for key, value in shuffled_combined_dataframes.items() if key in types}\n",
    "    \n",
    "    # Combine filtered DataFrames\n",
    "    combined_df = pd.concat(filtered_dataframes.values())\n",
    "    \n",
    "    # Sum the combined DataFrame by index\n",
    "    summed_df = combined_df.groupby(combined_df.index).sum()\n",
    "    \n",
    "    # Validate result\n",
    "    result = nrrd_for_validation(summed_df, parcellation_annotation, CCFv3)\n",
    "    \n",
    "    # Clean up\n",
    "    del combined_df, summed_df, filtered_dataframes\n",
    "\n",
    "    return (result, file_name)\n",
    "\n",
    "def main():\n",
    "    # Define the parameters for each process\n",
    "    tasks = [\n",
    "        (neurontypes, \"total_neuron_densities\"),\n",
    "        (exctypes, \"total_excitatory_densities\"),\n",
    "        (inhtypes, \"total_inhibitory_densities\"),\n",
    "        (astrotypes, \"total_astrotypes_densities\"),\n",
    "        (microglia, \"total_microglia_densities\"),\n",
    "        (oligos, \"total_oligocyte_densities\"),\n",
    "        (glia, \"total_glia_densities\"),\n",
    "        (exci_inhib_sum, \"total_excinh_densities\"),\n",
    "        (celltypes, \"total_celltypes_densities\"),\n",
    "    ]\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        results = pool.starmap(process_type, tasks)\n",
    "    \n",
    "    # Sequentially write the .nrrd files to avoid concurrent writes\n",
    "    for result, file_name in results:\n",
    "        nrrd.write(f\"{root_folder}{file_name}.nrrd\", result)\n",
    "        print(f\"{root_folder}{file_name}.nrrd\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97208254-2ea2-4f1b-a307-38c5a37c89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add scaling parameter to df_expected too by loading scaled total density files and search for corresponding values (computed earlier)\n",
    "\n",
    "# data_folder = \"/gpfs/bbp.cscs.ch/project/proj84/piluso/share/general/warped_augmented_CCFv3/\"\n",
    "# #CCFv3a, _ = nrrd.read(f'{data_folder}annotation_25_2022_CCFv3a.nrrd')\n",
    "# CCFv3a, _ = nrrd.read(\"/gpfs/bbp.cscs.ch/data/project/proj84/atlas_pipeline_runs/2024-05-15T22:44:26+02:00/annotation_ccfv3_l23split_barrelsplit_validated.nrrd\")\n",
    "\n",
    "\n",
    "#NEURON\n",
    "filename = \"total_neuron_densities.nrrd\"\n",
    "full_path = os.path.join(root_folder, filename)\n",
    "if os.path.isfile(full_path):\n",
    "    neuron_total, _ = nrrd.read(full_path)\n",
    "\n",
    "#EXC\n",
    "filename = \"total_excitatory_densities.nrrd\"\n",
    "full_path = os.path.join(root_folder, filename)\n",
    "if os.path.isfile(full_path):\n",
    "    exc_total, _ = nrrd.read(full_path)\n",
    "#INH        \n",
    "filename = \"total_inhibitory_densities.nrrd\"\n",
    "full_path = os.path.join(root_folder, filename)\n",
    "if os.path.isfile(full_path):\n",
    "    inh_total, _ = nrrd.read(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de93566-fb5a-42f2-b1bd-a1d30efcdff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((566, 320, 456), (566, 320, 456), (566, 320, 456))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_total.shape, exc_total.shape, inh_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e2587b-b0d0-41dd-80d3-59135a10ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.7 ms, sys: 875 ms, total: 943 ms\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "''' Calculating scaling parameters: We divide by \"mean expected / transcriptomic mean densities\". '''\n",
    "\n",
    "# Arrays to process\n",
    "arrays = {\n",
    "    'neurons_mm3': neuron_total,\n",
    "    'inh_mm3': inh_total,\n",
    "    'exc_mm3': exc_total,\n",
    "}\n",
    "\n",
    "# Worker function to process each row\n",
    "def process_row(row):\n",
    "    id_ = row['id'] \n",
    "    acr = row['acronym']\n",
    "    name = row['Region']\n",
    "    ids = row['children']\n",
    "    ids.append(id_) #Make sure you add the original id_ to the children\n",
    "    \n",
    "    \n",
    "    # Create a mask for the current id list\n",
    "    mask = np.isin(CCFv3, [ids])\n",
    "    \n",
    "    # Calculate mean/median values (can place NaN or None in the regions not present in CCfV3)\n",
    "    # mean_values = {name: arr[mask].mean() if arr[mask].size > \n",
    "    #                0 else np.nan for name, arr in arrays.items()}\n",
    "    mean_values = {name: arr[mask].mean() if arr[mask].size > \n",
    "                   0 else None for name, arr in arrays.items()}\n",
    "    # mean_values = {name: np.median(arr[mask]) if arr[mask].size > \n",
    "    #                0 else None for name, arr in arrays.items()}\n",
    "\n",
    "    # Update row with ratio values\n",
    "    if mean_values['exc_mm3'] is not np.nan and isinstance(row['exc_mm3'], (int, float)):\n",
    "        row['exc_mm3_ratio'] = row['exc_mm3'] / mean_values['exc_mm3']\n",
    "    if mean_values['inh_mm3'] is not np.nan and isinstance(row['inh_mm3'], (int, float)):\n",
    "        row['inh_mm3_ratio'] = row['inh_mm3'] / mean_values['inh_mm3']\n",
    "    if mean_values['neurons_mm3'] is not np.nan and isinstance(row['neurons_mm3'], (int, float)):\n",
    "        row['neurons_mm3_ratio'] = row['neurons_mm3'] / mean_values['neurons_mm3']\n",
    "    \n",
    "    return row\n",
    "    \n",
    "    \n",
    "# Use multiprocessing Pool to process rows in parallel\n",
    "if __name__ == '__main__':\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        new_data = pool.map(process_row, [row for _, row in df_expected.iterrows()])\n",
    "    \n",
    "    # Convert the list of rows back to a DataFrame\n",
    "    df_expected = pd.DataFrame(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a39061-9714-4d3f-9c50-8bc37e7463ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR  is not a leaf region recognised by transcriptomics data, it will be split to its leaves\n",
      "30 21 51\n"
     ]
    }
   ],
   "source": [
    "#import ast \n",
    "#Note that we will scale the scaled data: scaled_combined_result_dataframes\n",
    "combined_result_dataframes_copy = copy.deepcopy(combined_result_dataframes)\n",
    "\n",
    "\n",
    "for reg in df_expected['acronym'].values:\n",
    "    concatenated_result = np.array(df_expected['acronym'].values) \n",
    "    if reg not in list(combined_result_dataframes_copy.keys()):\n",
    "        print(reg, \" is not a leaf region recognised by transcriptomics data, it will be split to its leaves\")\n",
    "        #leaves_list = df_expected[df_expected['acronym'] == reg]['children'].iat[0]\n",
    "        acr_list_str = df_expected[df_expected['acronym'] == reg]['acr_list'].iat[0]\n",
    "        acr_list = ast.literal_eval(acr_list_str)\n",
    "\n",
    "        # Concatenate the array and the list to cover the full area where secondary scaling takes effect\n",
    "        # We combine the leaf regions with the list of regions non leaf regions\n",
    "        concatenated_result = np.concatenate((concatenated_result, acr_list))\n",
    "        print(len(acr_list), len(df_expected['acronym'].values), len(concatenated_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819c9abd-54d5-4001-9d49-c9e5c934781c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_result_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aad49d4-06c9-4b83-907e-a9fdc55b9c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA  is a leaf region which will be scaled.\n",
      "ACB  is a leaf region which will be scaled.\n",
      "BA  is a leaf region which will be scaled.\n",
      "CA1slm  is a leaf region which will be scaled.\n",
      "CA1so  is a leaf region which will be scaled.\n",
      "CA1sp  is a leaf region which will be scaled.\n",
      "CA1sr  is a leaf region which will be scaled.\n",
      "CA2slm  is a leaf region which will be scaled.\n",
      "CA2so  is a leaf region which will be scaled.\n",
      "CA2sp  is a leaf region which will be scaled.\n",
      "CA2sr  is a leaf region which will be scaled.\n",
      "CA3slm  is a leaf region which will be scaled.\n",
      "CA3so  is a leaf region which will be scaled.\n",
      "CA3sp  is a leaf region which will be scaled.\n",
      "CA3sr  is a leaf region which will be scaled.\n",
      "CEAc  is a leaf region which will be scaled.\n",
      "CEAl  is a leaf region which will be scaled.\n",
      "CEAm  is a leaf region which will be scaled.\n",
      "CP  is a leaf region which will be scaled.\n",
      "FS  is a leaf region which will be scaled.\n",
      "IA  is a leaf region which will be scaled.\n",
      "LSc  is a leaf region which will be scaled.\n",
      "LSr  is a leaf region which will be scaled.\n",
      "LSv  is a leaf region which will be scaled.\n",
      "MOBglomerularlayer  is a leaf region which will be scaled.\n",
      "MOBgr  is a leaf region which will be scaled.\n",
      "MOBipl  is a leaf region which will be scaled.\n",
      "MOBmi  is a leaf region which will be scaled.\n",
      "MOBopl  is a leaf region which will be scaled.\n",
      "PO  is a leaf region which will be scaled.\n",
      "SF  is a leaf region which will be scaled.\n",
      "SH  is a leaf region which will be scaled.\n",
      "VPL  is a leaf region which will be scaled.\n",
      "VPM  is a leaf region which will be scaled.\n",
      "CPU times: user 97.6 ms, sys: 1.8 ms, total: 99.4 ms\n",
      "Wall time: 97.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# del scaled_combined_result_dataframes\n",
    "# We create the dict of dfs anew!\n",
    "scaled_combined_result_dataframes = {}\n",
    "\n",
    "#Do the 2ndary scaling: we check every transcriptomic region and scale their cell types if needed: \n",
    "for key, df in combined_result_dataframes_copy.items():\n",
    "    if key in concatenated_result:\n",
    "        print(key, \" is a leaf region which will be scaled.\")\n",
    "        # We find the row where this region is listed (can be a substring if the row is not a leaf region)\n",
    "        matches = df_expected.map(lambda x: key in str(x))\n",
    "        rows_with_substring = df_expected[matches.any(axis=1)]\n",
    "        # If the region has a non-NAN value in the ratios, we multiply that type of cells with the ratio\n",
    "        if not rows_with_substring.empty and pd.notna(rows_with_substring['exc_mm3_ratio'].iloc[0]):\n",
    "            mask = df.index.isin(exctypes)\n",
    "            df.loc[mask, 'density_mm3'] *= rows_with_substring['exc_mm3_ratio'].iloc[0]\n",
    "            scaled_combined_result_dataframes[key] = df\n",
    "        #break\n",
    "        if not rows_with_substring.empty and pd.notna(rows_with_substring['inh_mm3_ratio'].iloc[0]):\n",
    "            mask = df.index.isin(inhtypes)\n",
    "            df.loc[mask, 'density_mm3'] *= rows_with_substring['inh_mm3_ratio'].iloc[0]\n",
    "            scaled_combined_result_dataframes[key] = df\n",
    "\n",
    "        if not rows_with_substring.empty and pd.notna(rows_with_substring['neurons_mm3_ratio'].iloc[0]):\n",
    "            mask = df.index.isin(neurontypes)\n",
    "            df.loc[mask, 'density_mm3'] *= rows_with_substring['neurons_mm3_ratio'].iloc[0]\n",
    "            scaled_combined_result_dataframes[key] = df\n",
    "            #break\n",
    "        #print(\"Deleting data for \" + key)\n",
    "        del key, df        \n",
    "    else:\n",
    "       #In this case we don't have to scale\n",
    "        scaled_combined_result_dataframes[key] = df\n",
    "        del key, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a10e0741-a604-46d7-934e-b9eea8052f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scaled_combined_result_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c20fe5-13c9-4b48-928c-bf7876e77e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'total_cells' is your dictionary of DataFrames\n",
    "with open(f'{root_folder}scaled_densities_local.pickle', 'wb') as f:\n",
    "    pickle.dump(scaled_combined_result_dataframes, f)\n",
    "\n",
    "print(f\"Saved here: {root_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fd7c96a-7c5f-4886-8117-00dacffd32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del scaled_combined_result_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21ee6c-495a-44d8-8fbd-112c14ea60f4",
   "metadata": {},
   "source": [
    "# Global scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9f368-25f4-4281-a5f9-644c88d444b7",
   "metadata": {},
   "source": [
    "This notebook starts after data was created from MERFISH slices and stored as csv and pickle files. We examine scaling individual cell groups based on literature: https://www.pnas.org/doi/full/10.1073/pnas.0604911103\n",
    "\n",
    "Total cells (M): 108.69 ± 16.25\n",
    "\n",
    "Total neurons (M): 70.89 ± 10.41\n",
    "\n",
    "CCTX: 17.8 ± 3.4% of neurons (this is not the isocortex)\n",
    "\n",
    "CRB: 59.0 ± 5.0% of neurons\n",
    "\n",
    "\"Cerebral cortex\" 688 = \"Cortical plate\" 695 + \"Cortical subplate\" 703\n",
    "\"Cerebral nuclei is not part of the cer.ctx)\n",
    "Cerebral cortex is not the Cerebellar cortex (the Cerebellar cortex is part of the Cerebellum)\n",
    "From test_tutorial_cerebellum.ipynb we decided to only scale exc cells (ie granular layer) in the CRB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4bf06f2-218f-48f7-ba77-b20261824750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pickle file.\n"
     ]
    }
   ],
   "source": [
    "#If we want we can load the previous step\n",
    "\n",
    "file = os.path.join( root_folder, 'scaled_densities_local.pickle' )\n",
    "#file = os.path.join( root_folder, 'non_scaled_densities_new.pickle' )\n",
    "\n",
    "#Load region id volumes from volume_calc_from_template.ipynb\n",
    "with open(file, 'rb') as pickle_file:\n",
    "    scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "print(\"Loaded pickle file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b096194-d149-494f-aeaa-88c0bf3b4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons only: \n",
      "CTX: 12,618,420 CRB: 41,825,100 REST: 16,446,480\n"
     ]
    }
   ],
   "source": [
    "#in this cell we just give ground truth cell counts from the paper\n",
    "total_cells = 108.69 * 1_000_000\n",
    "total_neurons = 70.89 * 1_000_000\n",
    "total_ctx = total_neurons * 17.8 / 100 #17.8% of the total neurons\n",
    "total_crb = total_neurons * 59.0 / 100 #59% of the total neurons\n",
    "total_rest = total_neurons * ( 100 - 59.0 -17.8 )/ 100\n",
    "print(\"Neurons only: \")\n",
    "print(\"CTX: \" '{:,.0f}'.format(total_ctx,), \"CRB: \" '{:,.0f}'.format(total_crb),\"REST: \" '{:,.0f}'.format(total_rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f9bcfea-11b5-4479-97d7-e15560ac3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume of the voxel in mm^3: 1.5625e-05\n",
      "Volume of the voxel in mm^3: 1e-06\n"
     ]
    }
   ],
   "source": [
    "#Next we have to convert densities to cell counts for the scaling. This way we can measure the scaling ratio between the current densities and the desired ones. For this we need voxel size\n",
    "\n",
    "# Edge length of the voxel in millimeters\n",
    "edge_length_mm = 25.0 / 1000.0  # Convert micrometers to millimeters\n",
    "\n",
    "# Calculate the volume of the voxel in cubic millimeters\n",
    "volume25_mm3 = round(edge_length_mm ** 3, 10)\n",
    "\n",
    "print(\"Volume of the voxel in mm^3:\", volume25_mm3)\n",
    "\n",
    "# Edge length of the voxel in millimeters\n",
    "edge_length_mm = 10.0 / 1000.0  # Convert micrometers to millimeters\n",
    "\n",
    "# Calculate the volume of the voxel in cubic millimeters\n",
    "volume10_mm3 = round(edge_length_mm ** 3, 10)\n",
    "\n",
    "print(\"Volume of the voxel in mm^3:\", volume10_mm3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4314351-d7b9-44bd-a4cc-e805134d4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation vol is 25 um resolution but it is larger than the Allen Institute's CCFv3\n",
      "CPU times: user 40.3 s, sys: 5.16 ms, total: 40.3 s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "total_cells = {}\n",
    "\n",
    "if CCFv3.shape == (528, 320, 456):\n",
    "    print(\"annotation vol is 25 um resolution\")\n",
    "    for region_name, df in scaled_combined_result_dataframes.items():\n",
    "        # Get the corresponding volume for the region\n",
    "        label = volumes.loc[volumes['cluster_as_filename'] == region_name, 'parcellation_label'].values[0]\n",
    "        id_ = int(label.rsplit('-', 1)[-1])\n",
    "        #id_ = region_map.find(region_name, attr='name', ignore_case=True, with_descendants=False)\n",
    "        volume = np.count_nonzero(CCFv3 == id_) * volume25_mm3\n",
    "        #print(region_name, label, id_, f\"{volume:.6f}\")\n",
    "        # Calculate total cells by multiplying density by volume\n",
    "        total_cells[region_name] = pd.DataFrame({'total_cellnumber': df['density_mm3'] * volume})\n",
    "\n",
    "elif CCFv3.shape == (566, 320, 456):\n",
    "    print(\"annotation vol is 25 um resolution but it is larger than the Allen Institute's CCFv3\")\n",
    "    for region_name, df in scaled_combined_result_dataframes.items():\n",
    "        # Get the corresponding volume for the region\n",
    "        label = volumes.loc[volumes['cluster_as_filename'] == region_name, 'parcellation_label'].values[0]\n",
    "        id_ = int(label.rsplit('-', 1)[-1])\n",
    "        #id_ = region_map.find(region_name, attr='name', ignore_case=True, with_descendants=False)\n",
    "        volume = np.count_nonzero(CCFv3 == id_) * volume25_mm3\n",
    "        #print(region_name, label, id_, f\"{volume:.6f}\")\n",
    "        # Calculate total cells by multiplying density by volume\n",
    "        total_cells[region_name] = pd.DataFrame({'total_cellnumber': df['density_mm3'] * volume})\n",
    "\n",
    "elif CCFv3.shape == (1320, 800, 1140) or CCFv3.shape == (1140, 800, 1320):\n",
    "    print(\"*** annotation vol is 10 um resolution *** \")\n",
    "    for region_name, df in scaled_combined_result_dataframes.items():\n",
    "        # Get the corresponding volume for the region\n",
    "        label = volumes.loc[volumes['cluster_as_filename'] == region_name, 'parcellation_label'].values[0]\n",
    "        id_ = int(label.rsplit('-', 1)[-1])\n",
    "        #id_ = region_map.find(region_name, attr='name', ignore_case=True, with_descendants=False)\n",
    "        #print(region_name, label, id_)\n",
    "        \n",
    "        if CCFv3.shape == (1320, 800, 1140):\n",
    "            volume = np.count_nonzero(CCFv3 == id_) * volume10_mm3\n",
    "        elif CCFv3.shape == (1140, 800, 1320):\n",
    "            volume = np.count_nonzero(CCFv3 == id_) * volume10_mm3 \n",
    "        # Calculate total cells by multiplying density by volume\n",
    "        print(region_name, label, id_, f\"{volume:.6f}\")\n",
    "        total_cells[region_name] = pd.DataFrame({'total_cellnumber': df['density_mm3'] * volume})\n",
    "else:\n",
    "    print(\"*** Annotation volume shape is not understood. *** \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa0e0a6-1577-49fd-89c8-099ca43a737d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5194d42c-86db-4635-ae8b-c872f199f282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/\n"
     ]
    }
   ],
   "source": [
    "#We can save the file just in case. \n",
    "\n",
    "# Assuming 'total_cells' is your dictionary of DataFrames\n",
    "with open(f'{root_folder}scaled_total_cells.pickle', 'wb') as f:\n",
    "    pickle.dump(total_cells, f)\n",
    "\n",
    "print(f\"Saved here: {root_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c93af0-cfb1-43c0-adf0-cd6acae9cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to isolate specific regions from the others to scale them differently\n",
    "\n",
    "isocortex = (\n",
    "    region_map.find(\"Isocortex\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"Entorhinal area\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"Piriform area\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "cerebellum = region_map.find(\n",
    "        \"Cerebellum\", attr=\"name\", with_descendants=True\n",
    "    ) | region_map.find(\"arbor vitae\", attr=\"name\", with_descendants=True)\n",
    "\n",
    "fiber_tracts_ids = (\n",
    "    region_map.find(\"fiber tracts\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"grooves\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"ventricular systems\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "hippocampus = (\n",
    "    region_map.find(\"Hippocampal region\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "hippocampal_formation = (\n",
    "    region_map.find(\"Hippocampal formation\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "thalamus = (\n",
    "    region_map.find(\"Thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "striatum = (\n",
    "    region_map.find(\"Striatum\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "VPL = (\n",
    "    region_map.find(\"Ventral posterolateral nucleus of the thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "LGd = (\n",
    "    region_map.find(\"Dorsal part of the lateral geniculate complex\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "VPM = (\n",
    "    region_map.find(\"Ventral posteromedial nucleus of the thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "MOB = (\n",
    "    region_map.find(\"Main olfactory bulb\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "\n",
    "#This cell will complement the two cells above\n",
    "Hindbrain = (\n",
    "    region_map.find(\"Hindbrain\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "Midbrain = (\n",
    "    region_map.find(\"Midbrain\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "Interbrain = (\n",
    "    region_map.find(\"Interbrain\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "Olfactoryareas = (\n",
    "    region_map.find(\"Olfactory areas\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "Corticalsubplate = (\n",
    "    region_map.find(\"Cortical subplate\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "Cerebralnuclei = (\n",
    "    region_map.find(\"Cerebral nuclei\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "cerebral_cortex = (\n",
    "    region_map.find(\"Cerebral cortex\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "wholebrain = (\n",
    "    region_map.find(\"root\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "\n",
    "#This is the final dictionary of all region names and their region ids\n",
    "areas = {\n",
    "    'wholebrain': wholebrain, 'Cerebral cortex': cerebral_cortex,\n",
    "    'isocortex': isocortex, 'cerebellum': cerebellum, 'fiber_tracts_ids': fiber_tracts_ids, \n",
    "    'hippocampus': hippocampus, 'hippocampal_formation':hippocampal_formation, \n",
    "    'thalamus': thalamus, 'striatum': striatum, 'VPL': VPL, 'LGd': LGd, 'VPM': VPM, 'MOB': MOB,\n",
    "    'Olfactory areas': Olfactoryareas, 'Cortical subplate': Corticalsubplate, \n",
    "    'Cerebral nuclei': Cerebralnuclei, 'Interbrain': Interbrain , 'Midbrain': Midbrain, \n",
    "    'Interbrain': Interbrain\n",
    "}\n",
    "\n",
    "areas_min = {\n",
    "    'wholebrain': wholebrain, 'Cerebral cortex': cerebral_cortex, 'cerebellum': cerebellum,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b565fe1-6013-48f9-a336-455566f2136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions _covered_ in the brain / total cells dict of df: 703\n",
      "Region was found, will remove: unassigned.\n",
      "For now we are removing the region unassigned as we cannot place it in the hierarchy.json system\n",
      "Region was found, will remove brain-unassigned.\n",
      "For now we are removing the region brain-unassigned as we cannot place it in the hierarchy.json system\n",
      "Remaining regions: 701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(708, 19)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have to filter unassigned regions since we don't know enough of them.\n",
    "\n",
    "regions = list(total_cells.keys())\n",
    "print(f'Regions _covered_ in the brain / total cells dict of df: {len(regions)}')\n",
    "# Assuming 'my_list' is your list\n",
    "if 'unassigned' in regions:\n",
    "    print(\"Region was found, will remove: unassigned.\")\n",
    "    regions.remove('unassigned')\n",
    "    print(\"For now we are removing the region unassigned as we cannot place it in the hierarchy.json system\")\n",
    "    \n",
    "if 'brainunassigned' in regions:\n",
    "    print(\"Region was found, will remove brain-unassigned.\")\n",
    "    regions.remove('brainunassigned')\n",
    "    print(\"For now we are removing the region brain-unassigned as we cannot place it in the hierarchy.json system\")\n",
    "\n",
    "if 'brain-unassigned' in regions:\n",
    "    print(\"Region was found, will remove brain-unassigned.\")\n",
    "    regions.remove('brain-unassigned')\n",
    "    print(\"For now we are removing the region brain-unassigned as we cannot place it in the hierarchy.json system\") \n",
    "    \n",
    "print(f\"Remaining regions: {len(regions)}\")\n",
    "region_info = parcellation_annotation[parcellation_annotation['cluster_as_filename'].isin(regions)]\n",
    "# region_info = parcellation_annotation[parcellation_annotation['parcellation_term_acronym'].isin(regions)]\n",
    "\n",
    "region_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409e7430-31d0-4e04-a33c-60aa7b503edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brain area wholebrain has 5156 different cell types\n",
      "Total sum of neurons: 53,360,742\n",
      "The brain area Cerebral cortex has 1800 different cell types\n",
      "Total sum of neurons: 33,502,874\n",
      "The brain area cerebellum has 1116 different cell types\n",
      "Total sum of neurons: 3,939,848\n"
     ]
    }
   ],
   "source": [
    "#Next we can calculate total cell numbers overall and in specific regions\n",
    "\n",
    "total_cells_copy = copy.deepcopy(total_cells) \n",
    "ground_truth_cell_numbers = []\n",
    "\n",
    "for key, df in total_cells_copy.items():\n",
    "    total_cells_copy[key] = df[df.index.isin(neurontypes)]\n",
    "\n",
    "for key, value in areas_min.items():\n",
    "    \n",
    "    keys_to_drop = ['unassigned', 'brain-unassigned']\n",
    "    for region in regions:\n",
    "        \n",
    "        #filtered_data = region_info[(region_info['cluster_as_filename'] == region) &(region_info['parcellation_term_set_name'] == 'substructure')]\n",
    "        filtered_data = parcellation_annotation[(parcellation_annotation['cluster_as_filename'] == region) &(region_info['parcellation_term_set_name'] == 'substructure')]\n",
    "        id_ = filtered_data['label_numbers'].iloc[0]\n",
    "        \n",
    "        if int(id_) in value:\n",
    "            #print(region, id_, \"is in the set\")\n",
    "            None\n",
    "        else:\n",
    "            #print(id_, \"is not in the set\")\n",
    "            keys_to_drop.append(region)\n",
    "\n",
    "        #break    \n",
    "\n",
    "    # Create a new dictionary without the specified keys (all leaf regions and their cell types in 1 region)\n",
    "    total_cells_filtered = {key: value for key, value in total_cells_copy.items() if key not in keys_to_drop}\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    concatenated_df = pd.concat(total_cells_filtered.values())\n",
    "\n",
    "    # Group by all columns except the 'total_cells' column and sum the values\n",
    "    summed_df = concatenated_df.groupby(concatenated_df.index).sum()\n",
    "    print(f\"The brain area {key} has {summed_df.shape[0]} different cell types\")\n",
    "    #print(\"Total sum of cells:\", '{:,.0f}'.format(summed_df['density_mm3'].sum()))\n",
    "    print(\"Total sum of neurons:\", '{:,.0f}'.format(summed_df['total_cellnumber'].sum()))\n",
    "    ground_truth_cell_numbers.append(summed_df['total_cellnumber'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "442b92b3-292d-45c8-a02d-881a6798eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE can store the values in a variable and get the scaling ratios\n",
    "mfish_ctx = ground_truth_cell_numbers[1]\n",
    "mfish_crb = ground_truth_cell_numbers[2]\n",
    "mfish_rest = ground_truth_cell_numbers[0] - mfish_ctx - mfish_crb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5d31d7d-79cc-4ab7-8a3b-e5c15f23b2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.66369430556719, 1061.5918036629878, 103.31988184651408)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling factors as percentage\n",
    "1/(mfish_ctx/total_ctx)*100, 1/(mfish_crb/total_crb)*100, 1/(mfish_rest/total_rest)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77eb7cb9-b3bd-469d-8df9-ba451b47f75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33502873.875372417, 12618420.0, 0.3766369430556719, 12618420.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfish_ctx, total_ctx, 1/(mfish_ctx/total_ctx), mfish_ctx*(1/(mfish_ctx/total_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dd6137d-183e-4ec7-96d9-26d78a6e9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa corpus callosum, anterior forceps\n",
      "icp inferior cerebellar peduncle\n",
      "sV sensory root of the trigeminal nerve\n",
      "scp superior cerebelar peduncles\n",
      "st stria terminalis\n",
      "309 54 340\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "#no CTX or CRB keys should be printed. Only regions with multiple solutions, which is taken care of elsewhere\n",
    "\n",
    "ctx_keys = []\n",
    "crb_keys = []\n",
    "else_keys = []\n",
    "for key in combined_result_dataframes.keys():\n",
    "    cluster_as_filename = parcellation_annotation[parcellation_annotation['cluster_as_filename'] == key]\n",
    "    cluster = cluster_as_filename[cluster_as_filename['parcellation_term_set_name'] == 'substructure']\n",
    "    #This != part is for ambiguous regions:\n",
    "    if cluster.shape[0] != 1:\n",
    "        print( key, cluster['parcellation_term_name'].values[0] )\n",
    "        for key_name in cluster['cluster_as_filename'].values:\n",
    "            # Add the element to else_keys list\n",
    "            else_keys.append(key_name)\n",
    "    elif cluster['label_numbers'].iloc[0] in cerebral_cortex:\n",
    "        ctx_keys.append(cluster['cluster_as_filename'].iloc[0])\n",
    "    elif cluster['label_numbers'].iloc[0] in cerebellum:\n",
    "        crb_keys.append(cluster['cluster_as_filename'].iloc[0])\n",
    "    else:\n",
    "        else_keys.append(cluster['cluster_as_filename'].iloc[0])\n",
    "\n",
    "#remove duplicates if there's any (in else there is)\n",
    "else_keys = list(dict.fromkeys(else_keys))\n",
    "crb_keys = list(dict.fromkeys(crb_keys))\n",
    "ctx_keys = list(dict.fromkeys(ctx_keys))\n",
    "\n",
    "print(len(ctx_keys), len(crb_keys), len(else_keys), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbcb62-3343-4125-a5d4-5608f6e41e2a",
   "metadata": {},
   "source": [
    "Scaling: We will scale the regions one by one according to their anatomical positions:\n",
    "\n",
    "- If the region was scaled earlier in the previous section we should not scale it anymore\n",
    "- In the CTX we can scale with the cortical scaling ratio\n",
    "- IN the CRB with the cerebellar scaling ratio\n",
    "- Everywhere else with the residual scaling ratio\n",
    "- Glia can be scaled by 1x (change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f113424-b31a-46d1-82d4-5bc03690c15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scaled_combined_result_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d7711d-6ee6-4e4e-a9e9-82466c207cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 559 ms, sys: 9 µs, total: 559 ms\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Scaling will take place\n",
    "combined_result_dataframes_copy = copy.deepcopy(scaled_combined_result_dataframes)\n",
    "# del scaled_combined_result_dataframes\n",
    "globally_scaled_combined_result_dfs = {}\n",
    "\n",
    "#Do the scaling: CTX: neurons only, CRB: EXC only, ELSE: neuron only\n",
    "for key, df in combined_result_dataframes_copy.items():\n",
    "    #Leave those regions which we have scaled already as they are\n",
    "    if key in concatenated_result:\n",
    "        globally_scaled_combined_result_dfs[key] = df\n",
    "       \n",
    "    #Scale only the neurons and glia:\n",
    "    elif key in ctx_keys:\n",
    "        # Select rows where the value is in neurontypes\n",
    "        mask = df.index.isin(neurontypes)\n",
    "        \n",
    "        # Divide only selected rows by scaling factor for CTX Neuron\n",
    "        df.loc[mask, 'density_mm3'] /= (mfish_ctx/total_ctx)\n",
    "        globally_scaled_combined_result_dfs[key] = df\n",
    "        del df, mask\n",
    "        \n",
    "    elif key in crb_keys:\n",
    "        # Select rows where the value is in  exc types\n",
    "        mask = df.index.isin(exctypes)\n",
    "        # Select rows where the value is in neuron+glia types (this was an attempt to see what glia numbers would be if changed)\n",
    "        # combined = np.concatenate((exctypes, glia))\n",
    "        # mask = df.index.isin(combined)\n",
    "        \n",
    "        # Divide only selected rows by scaling factor for CB EXC\n",
    "        df.loc[mask, 'density_mm3'] /= (mfish_crb/total_crb)\n",
    "        globally_scaled_combined_result_dfs[key] = df        \n",
    "        #print(df)\n",
    "        del df, mask\n",
    "    \n",
    "    elif key in else_keys:\n",
    "        # Select rows where the value is in neurontypes\n",
    "        mask = df.index.isin(neurontypes)\n",
    "        \n",
    "        # Divide only selected rows by scaling factor for REST\n",
    "        df.loc[mask, 'density_mm3'] /= (mfish_rest/total_rest)\n",
    "        globally_scaled_combined_result_dfs[key] = df        \n",
    "        del df, mask\n",
    "\n",
    "    else:\n",
    "        print(key, \"is an Issue!\")\n",
    "\n",
    "\n",
    "# # Check if multiplication took place\n",
    "# for key, df in combined_result_dataframes_copy.items():\n",
    "#     if key in ctx_keys:\n",
    "#         print(f'Original value for key {key}:')\n",
    "#         print(df['density_mm3'])\n",
    "        \n",
    "# for key, df in scaled_combined_result_dataframes.items():\n",
    "#     if key in ctx_keys:\n",
    "#         print(f'Copied value for key {key}:')\n",
    "#         print(df['density_mm3'])\n",
    "\n",
    "#del combined_result_dataframes_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2625aa93-05e8-4c25-b006-d3b61f1eb0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globally_scaled_combined_result_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995909a6-8a8c-4bd4-8feb-f737a1df0d0f",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6b6a369-0a8c-404d-b9a2-5ce0715a9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from voxcell import VoxelData\n",
    "from voxcell.nexus.voxelbrain import RegionMap\n",
    "from argparse import ArgumentParser\n",
    "import os, sys\n",
    "import warnings\n",
    "import inspect\n",
    "\n",
    "# Parse arguments\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"annotation\")\n",
    "parser.add_argument(\"hierarchy\")\n",
    "parser.add_argument(\"density_folder\")\n",
    "parser.add_argument(\"cell_density\")\n",
    "parser.add_argument(\"inhibitory_density_folder\")\n",
    "parser.add_argument(\"excitatory_ME_types_folder\")\n",
    "parser.add_argument(\"inhibitory_ME_types_folder\")\n",
    "parser.add_argument(\"excitatory_ME_types_transplant_folder\")\n",
    "parser.add_argument(\"inhibitory_ME_types_transplant_folder\")\n",
    "\n",
    "#brain_regions = '/gpfs/bbp.cscs.ch/home/veraszto/bbp_prod_files/2022/annotation_25.nrrd'\n",
    "brain_regions = '/home/piluso/data/00_allen_brain_atlas/ccfv2/annotation_25_ccfv2_combined.nrrd '\n",
    "\n",
    "#hierarchy_json = '/gpfs/bbp.cscs.ch/home/veraszto/bbp_prod_files/1.json'\n",
    "hierarchy_json ='/gpfs/bbp.cscs.ch/data/project/proj84/atlas_pipeline_runs/2024-05-15T22:44:26+02:00/hierarchy_ccfv3_l23split_barrelsplit.json'\n",
    "#cell_density = '/gpfs/bbp.cscs.ch/home/veraszto/leaves_only/cell_densities_correctednissl/neuron_density.nrrd'\n",
    "cell_density = '/gpfs/bbp.cscs.ch/home/piluso/cell_atlas/03_warped_annotation_fix_last/blue_brain_atlas_pipeline/leaves_only/overall_cell_density_correctednissl_validated.nrrd'\n",
    "#cell_density = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/total_celltypes_densities.nrrd'\n",
    "density_folder = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60b5259d-8b5f-4d2f-8dc7-6cf4052067bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Region filters setting\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRegion filters setting\")\n",
    "region_map = RegionMap.load_json(hierarchy_json)\n",
    "\n",
    "cerebellum = region_map.find(\n",
    "        \"Cerebellum\", attr=\"name\", with_descendants=True\n",
    "    ) | region_map.find(\"arbor vitae\", attr=\"name\", with_descendants=True)\n",
    "isocortex = (\n",
    "    region_map.find(\"Isocortex\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"Entorhinal area\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"Piriform area\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "fiber_tracts_ids = (\n",
    "    region_map.find(\"fiber tracts\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"grooves\", attr=\"name\", with_descendants=True)\n",
    "    | region_map.find(\"ventricular systems\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "hippocampus = (\n",
    "    region_map.find(\"Hippocampal region\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "hippocampal_formation = (\n",
    "    region_map.find(\"Hippocampal formation\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "thalamus = (\n",
    "    region_map.find(\"Thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "striatum = (\n",
    "    region_map.find(\"Striatum\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "VPL = (\n",
    "    region_map.find(\"Ventral posterolateral nucleus of the thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "LGd = (\n",
    "    region_map.find(\"Dorsal part of the lateral geniculate complex\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "VPM = (\n",
    "    region_map.find(\"Ventral posteromedial nucleus of the thalamus\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "MOB = (\n",
    "    region_map.find(\"Main olfactory bulb\", attr=\"name\", with_descendants=True)\n",
    ")\n",
    "\n",
    "if cerebellum == [] or \\\n",
    "isocortex == [] or \\\n",
    "fiber_tracts_ids == [] or \\\n",
    "hippocampus == [] or \\\n",
    "striatum == []:\n",
    "    raise ValueError(\"ERROR: some region filters return empty sets\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "annotation = VoxelData.load_nrrd('/gpfs/bbp.cscs.ch/data/project/proj84/piluso/share/general/warped_augmented_CCFv3/annotation_25_2022_CCFv3a.nrrd').raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a68126d4-cb76-4619-a673-baef758decf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Literature values setting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# # Literature values and initialization of paramters (to be moved into a configuration file)\n",
    "print(\"\\nLiterature values setting...\")\n",
    "voxel_volume = (25 * 1.0e-3)**3\n",
    "nb_voxels_ccfv2_2011 = 31984720 # 499.8 mm^3\n",
    "nb_voxels_ccfv2_2015 = 31992356 # = 499.9 mm^3\n",
    "nb_voxels_ccfv2_2015_fiber_tracts = 3525860 # = 55.1 mm^3\n",
    "nb_voxels_ccfv2_2015_whithout_fiber_tracts = nb_voxels_ccfv2_2015 - nb_voxels_ccfv2_2015_fiber_tracts # 444.8 mm^3\n",
    "nb_voxels_ccfv3_2015 = 32387385 # = 506.1 mm^3\n",
    "nb_voxels_ccfv3_2022 = 32261391 # = 504.1 mm^3\n",
    "nb_voxels_ccfv3_2022_fiber_tracts = 2980469 # = 46.6 mm^3\n",
    "nb_voxels_ccfv2_2022_whithout_fiber_tracts = nb_voxels_ccfv3_2022 - nb_voxels_ccfv3_2022_fiber_tracts # 457.5 mm^3\n",
    "nb_voxels_ccfv3_2022_augmented = None # Not yet available\n",
    "default_neuron_proportion = 0.16 # Default neuron density tolerance inheritated from total neuron tolerance when no data is available\n",
    "default_glia_proportion = 0.20 # Default glia density tolerance inheritated from total glia tolerance when no data is available\n",
    "default_cell_proportion = 0.18 # Default cell density tolerance inheritated from total cell tolerance when no data is available\n",
    "\n",
    "# Literature values for the whole brain (to be moved into a configuration file)\n",
    "wh_mouse_brain_vol_litt_m = 32570240 # = 508.91 mm^3 in Badea et al., 2007\n",
    "wh_mouse_brain_vol_tolerance_m = 1498880 # = 23.42 mm^3 (5%) in Badea et al., 2007wh_mouse_brain_vol_litt_m = 32570240 # = 508.91 mm^3 in Badea et al., 2007\n",
    "wh_mouse_brain_vol_litt = 508.91 # = 508.91 mm^3 in Badea et al., 2007\n",
    "wh_mouse_brain_vol_tolerance = 23.42 # = 23.42 mm^3 (5%) in Badea et al., 2007\n",
    "neuron_dens_fiber_tracts_litt = 0 # Rodarie et al., 2022\n",
    "neuron_dens_fiber_tracts_tolerance = 0 # Rodarie et al., 2022\n",
    "neuron_dens_litt = 71760000/wh_mouse_brain_vol_litt # = 67,870,000 + 3,890,000 Table 1 in Herculano-Houzel et al., 2011\n",
    "neuron_dens_tolerance = 11660000/wh_mouse_brain_vol_litt # = 10,410,000 + 1,250,000 (16%) Table 1 in Herculano-Houzel et al., 2011\n",
    "glia_dens_litt = 39320000/wh_mouse_brain_vol_litt # = 33,860,000 + 5,460,000 Table 1 in Herculano-Houzel et al., 2011\n",
    "glia_dens_tolerance = 7810000/wh_mouse_brain_vol_litt # = 6,660,000 + 1,150,000 (20%) Table 1 in Herculano-Houzel et al., 2011\n",
    "cell_dens_litt = 111080000/wh_mouse_brain_vol_litt # = 71,760,000 + 39,320,000 Table 1 in Herculano-Houzel et al., 2011 after summing neuron + glia\n",
    "cell_dens_tolerance = 19470000/wh_mouse_brain_vol_litt # = 11,660,000 + 7,810,000 (18%) Table 1 in Herculano-Houzel et al., 2011 after summing neuron + glia\n",
    "astrocyte_dens_litt = None # Not available\n",
    "astrocyte_dens_tolerance = None # Not available\n",
    "microglia_dens_litt = None # Not available\n",
    "microglia_dens_tolerance = None # Not available\n",
    "oligodendrocyte_dens_litt = None # Not available\n",
    "oligodendrocyte_dens_tolerance = None # Not available\n",
    "inhibitory_neuron_dens_litt = 14550000/wh_mouse_brain_vol_litt # Table 3 in Rodarie et al., 2022\n",
    "inhibitory_neuron_dens_tolerance = round(inhibitory_neuron_dens_litt * default_neuron_proportion) # default Not available /!\\\n",
    "excitatory_neurons_dens_litt = None # Not available\n",
    "excitatory_neurons_dens_tolerance = None # Not available\n",
    "pv_dens_litt = 2631372/wh_mouse_brain_vol_litt # = 5916 * 445 in measurements.csv (~17.6% * Inhibitory in Table 3) in Rodarie et al., 2022\n",
    "pv_dens_tolerance = 237072/wh_mouse_brain_vol_litt # = 533 (9%) in measurements.csv in Rodarie et al., 2022\n",
    "sst_dens_litt = 2253658/wh_mouse_brain_vol_litt# = 5067 * 445 (~15.8% * Inhibitory in Table 3) in Rodarie et al., 2022\n",
    "sst_dens_tolerance = 234498/wh_mouse_brain_vol_litt # = 527 (10%) in measurements.csv in Rodarie et al., 2022\n",
    "vip_dens_litt = 434928/wh_mouse_brain_vol_litt # = 978 * 445 (~3.1% * Inhibitory in Table 3) in Rodarie et al., 2022\n",
    "vip_dens_tolerance = 30616/wh_mouse_brain_vol_litt # 69 (7%) in measurements.csv in Rodarie et al., 2022\n",
    "rest_inhi_dens_litt = (inhibitory_neuron_dens_litt - (pv_dens_litt + sst_dens_litt + vip_dens_litt))/wh_mouse_brain_vol_litt # (~63.5% * Inhibitory in Table 3) in Rodarie et al., 2022\n",
    "rest_inhi_dens_tolerance = (inhibitory_neuron_dens_tolerance - (pv_dens_tolerance + sst_dens_tolerance + vip_dens_tolerance))/wh_mouse_brain_vol_litt # Rodarie et al., 2022\n",
    "\n",
    "# Region support in number of voxels\n",
    "isocortex_nb_vox = len(np.where(np.isin(annotation, list(isocortex)) != 0)[0])\n",
    "hippocampus_nb_vox = len(np.where(np.isin(annotation, list(hippocampus)) != 0)[0])\n",
    "striatum_nb_vox = len(np.where(np.isin(annotation, list(striatum)) != 0)[0])\n",
    "thalamus_nb_vox = len(np.where(np.isin(annotation, list(thalamus)) != 0)[0])\n",
    "VPL_nb_vox = len(np.where(np.isin(annotation, list(VPL)) != 0)[0])\n",
    "LGd_nb_vox = len(np.where(np.isin(annotation, list(LGd)) != 0)[0])\n",
    "VPM_nb_vox = len(np.where(np.isin(annotation, list(VPM)) != 0)[0])\n",
    "MOB_nb_vox = len(np.where(np.isin(annotation, list(MOB)) != 0)[0])\n",
    "hippocampal_formation_nb_vox = len(np.where(np.isin(annotation, list(hippocampal_formation)) != 0)[0])\n",
    "\n",
    "# Literature values for sub-regions (to be moved into a configuration file)\n",
    "isocortex_neuron_dens_litt = 2*5048837/(isocortex_nb_vox * voxel_volume) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_neuron_dens_tolerance = 2*412123/(isocortex_nb_vox * voxel_volume) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_glia_dens_litt = 2*6640234/(isocortex_nb_vox * voxel_volume) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_glia_dens_tolerance = 2*244643/(isocortex_nb_vox * voxel_volume) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_cell_dens_litt = (isocortex_neuron_dens_litt + isocortex_glia_dens_litt) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_cell_dens_tolerance = (isocortex_neuron_dens_tolerance + isocortex_glia_dens_tolerance) # Table 1 in Herculano_Houzel et al., 2013\n",
    "isocortex_oligo_dens_litt = 12500 # Table 1 in Erö et al., 2018\n",
    "isocortex_oligo_dens_tolerance = round(isocortex_oligo_dens_litt * default_glia_proportion) # default Not available /!\\\n",
    "isocortex_astro_dens_litt = 15696 # Table 1 in Erö et al., 2018\n",
    "isocortex_astro_dens_tolerance = round(isocortex_astro_dens_litt * default_glia_proportion) # default Not available /!\\\n",
    "isocortex_microglia_dens_litt = 6500 # Table 1 in Erö et al., 2018\n",
    "isocortex_microglia_dens_tolerance = round(isocortex_microglia_dens_litt * default_glia_proportion) # default Not available /!\\\n",
    "cerebellum_volume = 59.65 # mm^3 from Table 1 in Zhang et al., 2011\n",
    "cerebellum_volume_torlerance = 3.65 # mm^3 from Table 1 in Zhang et al., 2011\n",
    "cerebellum_neuron_dens_litt = 42220000/cerebellum_volume # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_neuron_dens_tolerance = 9280000/cerebellum_volume # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_glia_dens_litt = 6950000/cerebellum_volume # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_glia_dens_tolerance = 1500000/cerebellum_volume # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_cell_dens_litt = (cerebellum_neuron_dens_litt + cerebellum_glia_dens_litt) # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_cell_dens_tolerance = (cerebellum_neuron_dens_tolerance + cerebellum_glia_dens_tolerance) # Table 1 in Herculano_Houzel et al., 2011\n",
    "cerebellum_oligo_dens_litt = 13750 # = Table 1 in Erö et al., 2018\n",
    "cerebellum_oligo_dens_tolerance = 1768 # Table 1 in Erö et al., 2018\n",
    "cerebellum_astro_dens_litt = 1512 # Table 1 in Erö et al., 2018\n",
    "cerebellum_astro_dens_tolerance = round(cerebellum_astro_dens_litt * default_glia_proportion) # default Not available /!\\\n",
    "cerebellum_microglia_dens_litt = 8624 # Table 1 in Erö et al., 2018\n",
    "cerebellum_microglia_dens_tolerance = 659 # Table 1 in Erö et al., 2018\n",
    "hippocampus_neuron_dens_litt = 20848 # from Table 3 in Keller et al., 2018\n",
    "hippocampus_neuron_dens_tolerance = hippocampus_neuron_dens_litt * default_neuron_proportion # default Not available /!\\\n",
    "hippocampus_oligo_dens_litt = 9425 # Table 1 in Erö et al., 2018\n",
    "hippocampus_oligo_dens_tolerance = round(hippocampus_oligo_dens_litt * default_glia_proportion) # default Not available /!\\\n",
    "hippocampus_astro_dens_litt = 16737 # Table 1 in Erö et al., 2018\n",
    "hippocampus_astro_dens_tolerance = 10496 # Table 1 in Erö et al., 2018\n",
    "hippocampus_microglia_dens_litt = 3248 # Table 1 in Erö et al., 2018\n",
    "hippocampus_microglia_dens_tolerance = 1563 # Table 1 in Erö et al., 2018\n",
    "# striatum_neuron_dens_litt = 802679/(striatum_nb_vox * voxel_volume)  # Andsberg et al., 2001\n",
    "# striatum_neuron_dens_tolerance = 31665/(striatum_nb_vox * voxel_volume)# Andsberg et al., 2001\n",
    "striatum_neuron_dens_litt = 120110 # from Table 5 in Keller et al., 2018\n",
    "striatum_neuron_dens_tolerance = 31800 # from Table 5 in Keller et al., 2018\n",
    "striatum_oligo_dens_litt = 9950 # Table 1 in Erö et al., 2018\n",
    "striatum_oligo_dens_tolerance = 4036 # Table 1 in Erö et al., 2018\n",
    "striatum_astro_dens_litt = 9867 # Table 1 in Erö et al., 2018\n",
    "striatum_astro_dens_tolerance = 5547 # Table 1 in Erö et al., 2018\n",
    "striatum_microglia_dens_litt = 12101 # Table 1 in Erö et al., 2018\n",
    "striatum_microglia_dens_tolerance = 1930 # Table 1 in Erö et al., 2018\n",
    "LGd_neuron_dens_litt = 141000 # from Table 5 in Keller et al., 2018\n",
    "LGd_neuron_dens_tolerance = 24000 # from Table 5 in Keller et al., 2018\n",
    "thalamus_glia_dens_litt = 80966 # from Table 5 in Keller et al., 2018\n",
    "thalamus_glia_dens_tolerance = thalamus_glia_dens_litt * default_glia_proportion # default Not available /!\\\n",
    "thalamus_cell_dens_litt = 122450 # from Table 5 in Keller et al., 2018\n",
    "thalamus_cell_dens_tolerance = thalamus_cell_dens_litt * default_cell_proportion # default Not available /!\\\n",
    "VPM_neuron_dens_litt = 83100 # from Table 5 in Keller et al., 2018\n",
    "VPM_neuron_dens_tolerance = 7900 # from Table 5 in Keller et al., 2018\n",
    "VPL_neuron_dens_litt = 57466.92 # LNMC data from 2019 Thalamic Release Report pages 21-23 https://docs.google.com/document/d/1maQ8VIwaFeyOQpfUy6PoLcamp48NW9NUy6BPN_oLRQk/edit#heading=h.otpvbup5qa45\n",
    "# VPL_neuron_dens_tolerance = 5201.4 # LNMC data from 2019 Thalamic Release Report pages 21-23 https://docs.google.com/document/d/1maQ8VIwaFeyOQpfUy6PoLcamp48NW9NUy6BPN_oLRQk/edit#heading=h.otpvbup5qa45\n",
    "VPL_neuron_dens_tolerance = VPL_neuron_dens_litt * default_neuron_proportion # default Not available /!\\\n",
    "VPL_pv_dens_litt = 1238.528635\t# from Kim et al., 2017\n",
    "VPL_pv_dens_tolerance = 575.6900057 # from Kim et al., 2017\n",
    "VPL_sst_dens_litt = 1609.664213 # from Kim et al., 2017\n",
    "VPL_sst_dens_tolerance = 1186.589608 # from Kim et al., 2017\n",
    "VPL_vip_dens_litt = 0.746468649 # from Kim et al., 2017\n",
    "VPL_vip_dens_tolerance = 1.296087826 # from Kim et al., 2017\n",
    "MOB_cell_dens_litt = 383148 # from Tables in Parrish-Aungst et al., 2007\n",
    "MOB_cell_dens_tolerance = 27027 # from Tables in Parrish-Aungst et al., 2007\n",
    "MOB_neuron_dens_litt = 246422 # from Tables in Parrish-Aungst et al., 2007\n",
    "MOB_neuron_dens_tolerance = 17488 # from Tables in Parrish-Aungst et al., 2007\n",
    "MOB_glia_dens_litt = 136725 # from Tables in Parrish-Aungst et al., 2007\n",
    "MOB_glia_dens_tolerance = 9539 # from Tables in Parrish-Aungst et al., 2007\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "febe69bd-e1a3-4634-83e5-f5c4aa5bfddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launching the assertions and writing ouptut result in the log file...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLaunching the assertions and writing ouptut result in the log file...\")\n",
    "\n",
    "# with open(output_log_file_path, \"a\") as log_file:\n",
    "\n",
    "# Catching all prints to write them into a log file\n",
    "# sys.stdout = log_file\n",
    "\n",
    "# Functions\n",
    "def print_range_bar(value, min_value, max_value, bar_length=40):\n",
    "    \"\"\"\n",
    "    Printing the range bar of the given density compared to literature plus some statisics elements (std, z-score)\n",
    "    @method print_range_bar\n",
    "    @param {Float} value The input dentity value to assert\n",
    "    @param {Float} min_value The min density value set by literature\n",
    "    @param {Float} max_value The max density value set by literature\n",
    "    @param {Integer} bar_length The length of the bar to plot\n",
    "    @return {None}\n",
    "    \"\"\"\n",
    "    progress = (value - min_value) / (max_value - min_value)\n",
    "    progress = max(0, min(1, progress))\n",
    "    arrow = ' ' * int(round(bar_length * progress)) + '*'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    # Format the values as integers and display without decimal places\n",
    "    value = int(round(value))\n",
    "    min_value = int(round(min_value))\n",
    "    max_value = int(round(max_value))\n",
    "    mean_val = (min_value + max_value)/2\n",
    "    std = mean_val - min_value\n",
    "    z_score = round((value - mean_val)/std, 2)\n",
    "    std_percentage = round((mean_val - min_value)/mean_val*100, 1)\n",
    "    range_to_print = f'Range: [{arrow}{spaces}] {value}  | z-score = {z_score} |  -{std_percentage}% [{min_value}, {max_value}] +{std_percentage}%'\n",
    "    if value < min_value:\n",
    "        range_to_print = range_to_print.replace(\"[*\", \"*[\")\n",
    "        print(range_to_print)\n",
    "        # try:\n",
    "        #     print(\"diff = \" + str(mean_val - value) + \"  |   -\" + str(round(mean_val - value)/mean_val*100,1) + \"%\")\n",
    "        # except:\n",
    "        #     print(\"EXCEPTION: low limit\")\n",
    "        #     pass\n",
    "    elif value > max_value:\n",
    "        # try:\n",
    "        #     print(\"diff = \" + str(mean_val - value) + \"  |   +\" + str(round(mean_val - value)/mean_val*100,1) + \"%\")\n",
    "        # except:\n",
    "        #     print(\"EXCEPTION: high limit\")\n",
    "        #     pass\n",
    "        range_to_print = range_to_print.replace(\"*]\", \"]*\")\n",
    "        print(range_to_print)\n",
    "    else:\n",
    "        print(range_to_print)\n",
    "    return\n",
    "\n",
    "\n",
    "class DensityError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "def z_score_assertion(value = 0, min_value = 0, max_value = 0, assertion_message = \"\"):\n",
    "    \"\"\"\n",
    "    Asserting the z-score for a given assertion is in the right range from literature:\n",
    "        - |z| <= 1: VALIDATED\n",
    "        - 1 < |z| <= 2: WARNING\n",
    "        - |z| > 2: ERROR\n",
    "    @method z_score_assertion\n",
    "    @param {Float} value The input dentity value to assert\n",
    "    @param {Float} min_value The min density value set by literature\n",
    "    @param {Float} max_value The max density value set by literature\n",
    "    @param {String} assertion_message The assertion message to print if a Warning or an Error is raised\n",
    "    @return {None}\n",
    "    \"\"\"\n",
    "    mean_val = (min_value + max_value)/2\n",
    "    std = mean_val - min_value\n",
    "    z_score = round((value - mean_val)/std, 2)\n",
    "    if abs(z_score) <= 1:\n",
    "        print(\"Validated\")\n",
    "    elif (abs(z_score) > 1) and (abs(z_score) <= 2):\n",
    "        warnings.warn(assertion_message, UserWarning)\n",
    "        print(\"WARNING:\", assertion_message)\n",
    "    elif abs(z_score) > 2:\n",
    "        print(\"ERROR:\", assertion_message)\n",
    "        #raise DensityError(assertion_message)\n",
    "    else:\n",
    "        raise ValueError(\"Uknown value\")\n",
    "    return\n",
    "\n",
    "\n",
    "def z_score_assertion_sub_regions(value = 0, min_value = 0, max_value = 0, assertion_message = \"\"):\n",
    "    \"\"\"\n",
    "    Asserting the z-score for a given assertion is in the right range from literature:\n",
    "        - |z| <= 1: VALIDATED\n",
    "        - |z| > 1: WARNING\n",
    "    @method z_score_assertion_sub_regions\n",
    "    @param {Float} value The input dentity value to assert\n",
    "    @param {Float} min_value The min density value set by literature\n",
    "    @param {Float} max_value The max density value set by literature\n",
    "    @param {String} assertion_message The assertion message to print if a Warning is raised\n",
    "    @return {None}\n",
    "    \"\"\"\n",
    "    mean_val = (min_value + max_value)/2\n",
    "    std = mean_val - min_value\n",
    "    z_score = round((value - mean_val)/std, 2)\n",
    "    if abs(z_score) <= 1:\n",
    "        print(\"Validated\")\n",
    "    elif abs(z_score) > 1:\n",
    "        warnings.warn(assertion_message, UserWarning)\n",
    "        print(\"WARNING:\", assertion_message)\n",
    "    else:\n",
    "        raise ValueError(\"Uknown value\")\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def z_score_assertion_after_transplant(value = 0, min_value = 0, max_value = 0, assertion_message = \"\"):\n",
    "    \"\"\"\n",
    "    Asserting the z-score for a given assertion after transplant is in the right range from literature:\n",
    "        - |z| <= 1: VALIDATED\n",
    "        - |z| > 1: ERROR\n",
    "    @method z_score_assertion\n",
    "    @param {Float} value The input dentity value to assert\n",
    "    @param {Float} min_value The min density value set by literature\n",
    "    @param {Float} max_value The max density value set by literature\n",
    "    @param {String} assertion_message The assertion message to print if a Warning or an Error is raised\n",
    "    @return {None}\n",
    "    \"\"\"\n",
    "    mean_val = (min_value + max_value)/2\n",
    "    std = mean_val - min_value\n",
    "    z_score = round((value - mean_val)/std, 2)\n",
    "    if abs(z_score) <= 1:\n",
    "        print(\"Validated\")\n",
    "    elif (abs(z_score) > 1):\n",
    "        print(\"ERROR:\", assertion_message)\n",
    "        #raise DensityError(assertion_message)\n",
    "    else:\n",
    "        raise ValueError(\"Uknown value\")\n",
    "    return\n",
    "\n",
    "# =============================================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28d27b8e-f553-4555-9e69-c075b34c45bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assertion on total annotation volumetry (mm^3)\n",
      "Range: [                      *                 ] 32750610  | z-score = 0.12 |  -4.6% [31071360, 34069120] +4.6%\n",
      "Validated\n"
     ]
    }
   ],
   "source": [
    "# Assertion on the total volumetry of the annotation\n",
    "whole_brain_annotation_dens = len(np.where(annotation != 0)[0])\n",
    "annotation_dens_diff = abs(wh_mouse_brain_vol_litt_m - whole_brain_annotation_dens)\n",
    "print(\"\\nAssertion on total annotation volumetry (mm^3)\")\n",
    "print_range_bar(whole_brain_annotation_dens, wh_mouse_brain_vol_litt_m - wh_mouse_brain_vol_tolerance_m, wh_mouse_brain_vol_litt_m + wh_mouse_brain_vol_tolerance_m)\n",
    "assertion_message = \"total annotation volumetry out of literature range\"\n",
    "z_score_assertion(whole_brain_annotation_dens, wh_mouse_brain_vol_litt_m - wh_mouse_brain_vol_tolerance_m, wh_mouse_brain_vol_litt_m + wh_mouse_brain_vol_tolerance_m, assertion_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1488aae3-d2cb-4f8c-8623-d46d25e39727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_combined_dataframe\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8e23900-b4fe-4168-8bc9-c38f9fe5155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "0.6 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "0.7 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "0.8 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "0.9 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.1 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.2 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.3 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.4 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.5 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.6 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.7 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.8 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n",
      "1.9 factor is saved here: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations_3/\n"
     ]
    }
   ],
   "source": [
    "sc_f_inh = 1.9\n",
    "sc_f_glia = 0.75\n",
    "sc_f_exc = 0.75\n",
    "\n",
    "#0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,\n",
    "for sc_factor in [0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,1.7, 1.8, 1.9]:\n",
    "    #Scaling will take place\n",
    "    combined_result_dataframes_copy = copy.deepcopy(globally_scaled_combined_result_dfs)\n",
    "    # del scaled_combined_result_dataframes\n",
    "    globally_scaled_combined_result_dfs_2 = {}\n",
    "    \n",
    "    #Do the scaling:\n",
    "    for key, df in combined_result_dataframes_copy.items():\n",
    "        #Leave those regions which we have overwritten as they are\n",
    "        if key in concatenated_result:       \n",
    "    \n",
    "            # Select rows where the value is in glia\n",
    "            mask = df.index.isin(glia)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX\n",
    "            df.loc[mask, 'density_mm3'] /= sc_factor #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            \n",
    "        #Scale CORTEX:\n",
    "        elif key in ctx_keys:     \n",
    "            # Select rows where the value is in excitatory neurons\n",
    "            mask = df.index.isin(exctypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX GLIA\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_exc #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "            \n",
    "            # Select rows where the value is in glia\n",
    "            mask = df.index.isin(glia)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX GLIA\n",
    "            df.loc[mask, 'density_mm3'] /= sc_factor #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "    \n",
    "            # Select rows where the value is in  inh types\n",
    "            mask = df.index.isin(inhtypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX INH\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_inh\n",
    "            globally_scaled_combined_result_dfs_2[key] = df        \n",
    "            #print(df)\n",
    "            del df, mask\n",
    "\n",
    "        # Scale CEREBELLUM\n",
    "        elif key in crb_keys:\n",
    "            # Select rows where the value is in excitatory neurons\n",
    "            mask = df.index.isin(exctypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX EXC\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_exc #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "            \n",
    "            # Select rows where the value is in glia\n",
    "            mask = df.index.isin(glia)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CB GLIA\n",
    "            df.loc[mask, 'density_mm3'] /= sc_factor #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "    \n",
    "            # Select rows where the value is in  inh types\n",
    "            mask = df.index.isin(inhtypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CB INH\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_inh\n",
    "            globally_scaled_combined_result_dfs_2[key] = df        \n",
    "            #print(df)\n",
    "            del df, mask\n",
    "\n",
    "        # Scale REST of the BRAIN\n",
    "        elif key in else_keys:\n",
    "            # Select rows where the value is in excitatory neurons\n",
    "            mask = df.index.isin(exctypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for CTX EXC\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_exc #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "            \n",
    "            # Select rows where the value is in glia\n",
    "            mask = df.index.isin(glia)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for REST GLIA\n",
    "            df.loc[mask, 'density_mm3'] /= sc_factor #Change later\n",
    "            globally_scaled_combined_result_dfs_2[key] = df\n",
    "            del mask\n",
    "    \n",
    "            # Select rows where the value is in  inh types\n",
    "            mask = df.index.isin(inhtypes)\n",
    "            \n",
    "            # Divide only selected rows by scaling factor for REST INH\n",
    "            df.loc[mask, 'density_mm3'] /= sc_f_inh\n",
    "            globally_scaled_combined_result_dfs_2[key] = df        \n",
    "            #print(df)\n",
    "            del df, mask\n",
    "    \n",
    "        else:\n",
    "            print(key, \"is an Issue!\")\n",
    "\n",
    "    # Assuming 'total_cells' is your dictionary of DataFrames\n",
    "    with open(f'{root_folder}scaled_densities_new_{sc_factor}.pickle', 'wb') as f:\n",
    "        pickle.dump(globally_scaled_combined_result_dfs_2, f)\n",
    "    print(f\"{sc_factor} factor is saved here: {root_folder}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ea898e2-594a-4d76-a6f2-a6a4411a06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type(types, file_name):\n",
    "    # Filter DataFrames based on types\n",
    "    filtered_dataframes = {key: value for key, value in shuffled_combined_dataframes.items() if key in types}\n",
    "    \n",
    "    # Combine filtered DataFrames\n",
    "    combined_df = pd.concat(filtered_dataframes.values())\n",
    "    \n",
    "    # Sum the combined DataFrame by index\n",
    "    summed_df = combined_df.groupby(combined_df.index).sum()\n",
    "    \n",
    "    # Validate result\n",
    "    result = nrrd_for_validation(summed_df, parcellation_annotation, CCFv3)\n",
    "    \n",
    "    # Clean up\n",
    "    del combined_df, summed_df, filtered_dataframes\n",
    "    \n",
    "    return (result, file_name)\n",
    "\n",
    "def main():\n",
    "    # Define the parameters for each process\n",
    "    tasks = [\n",
    "        # (neurontypes, \"scaled_total_neuron_densities\"),\n",
    "        # (nonneurontypes, \"scaled_total_nonneuron_densities\"),\n",
    "        # (exctypes, \"scaled_total_excitatory_densities\"),\n",
    "        # (inhtypes, \"scaled_total_inhibitory_densities\"),\n",
    "        # (astrotypes, \"scaled_total_astrotypes_densities\"),\n",
    "        # (microglia, \"scaled_total_microglia_densities\"),\n",
    "        # (oligos, \"scaled_total_oligocyte_densities\"),\n",
    "        (glia, \"scaled_total_glia_densities\"),\n",
    "        # (exci_inhib_sum, \"scaled_total_excinh_densities\"),\n",
    "        # (celltypes, \"scaled_total_celltypes_densities\"),\n",
    "    ]    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        results = pool.starmap(process_type, tasks)\n",
    "    \n",
    "    # Sequentially write the .nrrd files to avoid concurrent writes\n",
    "    for result, file_name in results:\n",
    "        print(file_name)\n",
    "        nrrd.write(f\"{root_folder}{file_name}.nrrd\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bd400-7b1b-485f-8699-97a1a432cd5e",
   "metadata": {},
   "source": [
    "EXC VALIDATION WITH NEURONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c87fe21-611c-4acf-b7f1-698cb16087ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/ssd/slurmTmpFS/veraszto/2504452/ipykernel_62515/1828245804.py:78: UserWarning: total neuron densities out of literature range\n",
      "  warnings.warn(assertion_message, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:20\u001b[0m\n",
      "Cell \u001b[0;32mIn[49], line 35\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a multiprocessing Pool\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 35\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Sequentially write the .nrrd files to avoid concurrent writes\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result, file_name \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "log_file_path = f\"{root_folder}/log/EXC_print_messages.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    with redirect_stdout(log_file):\n",
    "        for sc_factor in [0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,1.7, 1.8, 1.9]: \n",
    "            try: del shuffled_combined_dataframes\n",
    "            except NameError: pass\n",
    "            \n",
    "            file = os.path.join(f'{root_folder}scaled_densities_new_{sc_factor}.pickle')\n",
    "            #Load region id volumes from volume_calc_from_template.ipynb\n",
    "            with open(file, 'rb') as pickle_file:\n",
    "                scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "        \n",
    "            print(\"\", file=log_file)\n",
    "            print(f\"Loaded pickle file for {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            shuffled_combined_dataframes = create_combined_dataframe(scaled_combined_result_dataframes) #[celltypes]region\n",
    "            print(\"Celltype/region dict is created.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            main()\n",
    "            print(\"Total EXC files for sc_factor is created.\", file=log_file)\n",
    "            #glia_total, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_glia_densities.nrrd\"))\n",
    "            neuron, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_neuron_densities.nrrd\"))\n",
    "            print(\"Total EXC file loaded.\", file=log_file)\n",
    "            log_file.flush()\n",
    "\n",
    "            # Assertion on total neuron densities\n",
    "            neuron_dens = np.sum(neuron) / whole_brain_annotation_dens # * voxel_volume\n",
    "            neuron_dens_diff = abs(neuron_dens_litt - neuron_dens)\n",
    "            print(\"\\nAssertion on total neuron densities (/mm^3)\")\n",
    "            print_range_bar(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance)\n",
    "            assertion_message = \"total neuron densities out of literature range\"\n",
    "            z_score_assertion(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance, assertion_message)            \n",
    "                 \n",
    "            print(f\"Finished {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659038-fc24-4066-b765-4b795ee67977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d2c3d8-f9bf-4787-a8ce-07ef90417a9d",
   "metadata": {},
   "source": [
    "GLIA VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80f0a8-d3a2-4d12-a0e2-9523d04771e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2154:\n",
      "Process ForkPoolWorker-2155:\n",
      "Process ForkPoolWorker-2142:\n",
      "Process ForkPoolWorker-2152:\n",
      "Process ForkPoolWorker-2147:\n",
      "Process ForkPoolWorker-2146:\n",
      "Process ForkPoolWorker-2151:\n",
      "Process ForkPoolWorker-2144:\n",
      "Process ForkPoolWorker-2145:\n",
      "Process ForkPoolWorker-2143:\n",
      "Process ForkPoolWorker-2159:\n",
      "Process ForkPoolWorker-2150:\n",
      "Process ForkPoolWorker-2157:\n",
      "Process ForkPoolWorker-2139:\n",
      "Process ForkPoolWorker-2160:\n",
      "Process ForkPoolWorker-2116:\n",
      "Process ForkPoolWorker-2141:\n",
      "Process ForkPoolWorker-2098:\n",
      "Process ForkPoolWorker-2120:\n",
      "Process ForkPoolWorker-2148:\n",
      "Process ForkPoolWorker-2149:\n",
      "Process ForkPoolWorker-2156:\n",
      "Process ForkPoolWorker-2153:\n",
      "Process ForkPoolWorker-2158:\n",
      "Process ForkPoolWorker-2138:\n",
      "Process ForkPoolWorker-2090:\n",
      "Process ForkPoolWorker-2089:\n",
      "Process ForkPoolWorker-2115:\n",
      "Process ForkPoolWorker-2091:\n",
      "Process ForkPoolWorker-2105:\n",
      "Process ForkPoolWorker-2088:\n",
      "Process ForkPoolWorker-2117:\n",
      "Process ForkPoolWorker-2122:\n",
      "Process ForkPoolWorker-2083:\n",
      "Process ForkPoolWorker-2099:\n",
      "Process ForkPoolWorker-2140:\n",
      "Process ForkPoolWorker-2104:\n",
      "Process ForkPoolWorker-2087:\n",
      "Process ForkPoolWorker-2110:\n",
      "Process ForkPoolWorker-2125:\n",
      "Process ForkPoolWorker-2096:\n",
      "Process ForkPoolWorker-2132:\n",
      "Process ForkPoolWorker-2085:\n",
      "Process ForkPoolWorker-2111:\n",
      "Process ForkPoolWorker-2112:\n",
      "Process ForkPoolWorker-2131:\n",
      "Process ForkPoolWorker-2107:\n",
      "Process ForkPoolWorker-2100:\n",
      "Process ForkPoolWorker-2108:\n",
      "Process ForkPoolWorker-2130:\n",
      "Process ForkPoolWorker-2102:\n",
      "Process ForkPoolWorker-2094:\n",
      "Process ForkPoolWorker-2109:\n",
      "Process ForkPoolWorker-2082:\n",
      "Process ForkPoolWorker-2114:\n",
      "Process ForkPoolWorker-2097:\n",
      "Process ForkPoolWorker-2092:\n",
      "Process ForkPoolWorker-2101:\n",
      "Process ForkPoolWorker-2093:\n",
      "Process ForkPoolWorker-2119:\n",
      "Process ForkPoolWorker-2133:\n",
      "Process ForkPoolWorker-2124:\n",
      "Process ForkPoolWorker-2134:\n",
      "Process ForkPoolWorker-2084:\n",
      "Process ForkPoolWorker-2095:\n",
      "Process ForkPoolWorker-2135:\n",
      "Process ForkPoolWorker-2103:\n",
      "Process ForkPoolWorker-2086:\n",
      "Process ForkPoolWorker-2128:\n",
      "Process ForkPoolWorker-2123:\n",
      "Process ForkPoolWorker-2129:\n",
      "Process ForkPoolWorker-2118:\n",
      "Process ForkPoolWorker-2126:\n",
      "Process ForkPoolWorker-2106:\n",
      "Process ForkPoolWorker-2113:\n",
      "Process ForkPoolWorker-2121:\n",
      "Process ForkPoolWorker-2127:\n",
      "Process ForkPoolWorker-2137:\n",
      "Process ForkPoolWorker-2136:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "log_file_path = f\"{root_folder}/log/glia_print_messages.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    with redirect_stdout(log_file):\n",
    "        for sc_factor in [0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,1.7, 1.8, 1.9]: \n",
    "            try: del shuffled_combined_dataframes\n",
    "            except NameError: pass\n",
    "            \n",
    "            file = os.path.join(f'{root_folder}scaled_densities_new_{sc_factor}.pickle')\n",
    "            #Load region id volumes from volume_calc_from_template.ipynb\n",
    "            with open(file, 'rb') as pickle_file:\n",
    "                scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "        \n",
    "            print(\"\", file=log_file)\n",
    "            print(f\"Loaded pickle file for {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            shuffled_combined_dataframes = create_combined_dataframe(scaled_combined_result_dataframes) #[celltypes]region\n",
    "            print(\"Celltype/region dict is created.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            main()\n",
    "            print(\"Total glia files for sc_factor is created.\", file=log_file)\n",
    "            glia_total, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_glia_densities.nrrd\"))\n",
    "            print(\"Total glia file loaded.\", file=log_file)\n",
    "            log_file.flush()\n",
    "\n",
    "            # Assertion on total glia densities\n",
    "            glia_dens = np.sum(glia_total) / whole_brain_annotation_dens # * voxel_volume\n",
    "            glia_dens_diff = abs(glia_dens_litt - glia_dens)\n",
    "            print(\"\\nAssertion on total glia densities (/mm^3)\")\n",
    "            print_range_bar(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance)\n",
    "            assertion_message = \"total glia densities out of literature range\"\n",
    "            z_score_assertion(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance, assertion_message)            \n",
    "        \n",
    "            print(f\"Finished {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0240c82-fc57-4057-8422-119f924b0a5d",
   "metadata": {},
   "source": [
    "INHIBITORY VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0d57f77-d35a-42e5-a551-c72f6f6e7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 45s, sys: 8.86 s, total: 2min 54s\n",
      "Wall time: 30min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,\n",
    "from contextlib import redirect_stdout\n",
    "log_file_path = f\"{root_folder}/log/inh_print_messages.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    with redirect_stdout(log_file):\n",
    "        for sc_factor in [1.7, 1.8, 1.9]: \n",
    "            try: del shuffled_combined_dataframes\n",
    "            except NameError: pass\n",
    "            \n",
    "            file = os.path.join(f'{root_folder}scaled_densities_new_{sc_factor}.pickle')\n",
    "            #Load region id volumes from volume_calc_from_template.ipynb\n",
    "            with open(file, 'rb') as pickle_file:\n",
    "                scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "        \n",
    "            print(\"\", file=log_file)\n",
    "            print(f\"Loaded pickle file for {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            shuffled_combined_dataframes = create_combined_dataframe(scaled_combined_result_dataframes) #[celltypes]region\n",
    "            print(\"Celltype/region dict is created.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            main()\n",
    "            print(\"Total INH files for sc_factor is created.\", file=log_file)\n",
    "            gad, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_inhibitory_densities.nrrd\"))\n",
    "            print(\"Total INH file loaded.\", file=log_file)\n",
    "            log_file.flush()\n",
    "            # Assertion on total inhibitory neuron densities\n",
    "            inhi_dens = np.sum(gad) / whole_brain_annotation_dens # * voxel_volume\n",
    "            diff_inhi = abs(inhibitory_neuron_dens_litt - inhi_dens)\n",
    "            print(\"\\nAssertion on total inhibitory neuron densities (/mm^3)\", file=log_file)\n",
    "            print(r\"/!\\ Tolerance set to default for neuron density\", file=log_file)\n",
    "            # print(\"/!\\ Default tolerance increased by a factor of 2.5\")\n",
    "            print_range_bar(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance)\n",
    "            # assert (diff_inhi <= inhibitory_neuron_dens_tolerance * 2.5) #, f\"diff_ini = \" + str(diff_inhi) + \"   |   tolerence = \" + str(inhibitory_neuron_dens_tolerance)\n",
    "            assertion_message = \"total inhibitory neuron densities out of literature range\"\n",
    "            z_score_assertion(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance, assertion_message)\n",
    "        \n",
    "            print(f\"Finished {sc_factor}.\", file=log_file)\n",
    "            log_file.flush()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af767a05-acc5-4d96-a0f2-a3c3eacb3c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0e548c88-aa8e-4c52-bb55-b0b807eecb7b",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "for sc_factor in [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]: \n",
    "    try: del shuffled_combined_dataframes\n",
    "    except NameError: pass\n",
    "    \n",
    "    file = os.path.join(f'{root_folder}scaled_densities_new_{sc_factor}.pickle')\n",
    "    #Load region id volumes from volume_calc_from_template.ipynb\n",
    "    with open(file, 'rb') as pickle_file:\n",
    "        scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "\n",
    "    print()\n",
    "    print(\"Loaded pickle file.\")\n",
    "    shuffled_combined_dataframes = create_combined_dataframe(scaled_combined_result_dataframes) #[celltypes]region\n",
    "\n",
    "    def main():\n",
    "        # Define the parameters for each process\n",
    "        tasks = [\n",
    "            (neurontypes, \"scaled_total_neuron_densities\"),\n",
    "            (nonneurontypes, \"scaled_total_nonneuron_densities\"),\n",
    "            (exctypes, \"scaled_total_excitatory_densities\"),\n",
    "            (inhtypes, \"scaled_total_inhibitory_densities\"),\n",
    "            (astrotypes, \"scaled_total_astrotypes_densities\"),\n",
    "            (microglia, \"scaled_total_microglia_densities\"),\n",
    "            (oligos, \"scaled_total_oligocyte_densities\"),\n",
    "            (glia, \"scaled_total_glia_densities\"),\n",
    "            (exci_inhib_sum, \"scaled_total_excinh_densities\"),\n",
    "            (celltypes, \"scaled_total_celltypes_densities\"),\n",
    "        ]    \n",
    "        # Create a multiprocessing Pool\n",
    "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            results = pool.starmap(process_type, tasks)\n",
    "        \n",
    "        # Sequentially write the .nrrd files to avoid concurrent writes\n",
    "        for result, file_name in results:\n",
    "            print(file_name)\n",
    "            nrrd.write(f\"{root_folder}{file_name}.nrrd\", result)\n",
    "\n",
    "    main()    \n",
    "\n",
    "    cell, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_celltypes_densities.nrrd\"))\n",
    "    neuron, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_neuron_densities.nrrd\"))\n",
    "    \n",
    "    exc, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_excitatory_densities.nrrd\"))\n",
    "    gad, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_inhibitory_densities.nrrd\"))\n",
    "    microglia, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_microglia_densities.nrrd\"))\n",
    "    nn, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_nonneuron_densities.nrrd\"))\n",
    "    olig, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_oligocyte_densities.nrrd\"))\n",
    "    astro, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_astrotypes_densities.nrrd\"))\n",
    "    glia, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_glia_densities.nrrd\"))\n",
    "    exci_inhib_sum, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_excinh_densities.nrrd\"))\n",
    "\n",
    "\n",
    "    z = 500\n",
    "    plt.imshow(cell[z], cmap='coolwarm');\n",
    "    # Define the coordinates\n",
    "    x = 222\n",
    "    y = 155\n",
    "    \n",
    "    name = region_map.get(annotation[z,y,x], \"name\", with_ascendants=False)\n",
    "    \n",
    "    plt.scatter(x, y, color='yellow', label='Point', s=10)\n",
    "    # Add labels and title\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(f'The id at this point is {annotation[z,y,x]}, {name}, the value is {cell[z,y,x]}')\n",
    "    # Show the plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(annotation[z,y,x], name, \"value\", cell[z,y,x],\"(\", z, y, x, \")\")\n",
    "\n",
    "    # Assertion on total cell densities\n",
    "    cell_dens = np.sum(cell) / whole_brain_annotation_dens# * voxel_volume\n",
    "    cell_dens_diff = abs(cell_dens_litt - cell_dens)\n",
    "    print(\"\\nAssertion on total cell densities (/mm^3)\")\n",
    "    print_range_bar(cell_dens, cell_dens_litt - cell_dens_tolerance, cell_dens_litt + cell_dens_tolerance)\n",
    "    assertion_message = \"total cell densities out of literature range\"\n",
    "    z_score_assertion(cell_dens, cell_dens_litt - cell_dens_tolerance, cell_dens_litt + cell_dens_tolerance, assertion_message)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Assertion on total neuron densities\n",
    "    neuron_dens = np.sum(neuron) / whole_brain_annotation_dens # * voxel_volume\n",
    "    neuron_dens_diff = abs(neuron_dens_litt - neuron_dens)\n",
    "    print(\"\\nAssertion on total neuron densities (/mm^3)\")\n",
    "    print_range_bar(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance)\n",
    "    assertion_message = \"total neuron densities out of literature range\"\n",
    "    z_score_assertion(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance, assertion_message)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Assertion on total glia densities\n",
    "    glia_dens = np.sum(glia) / whole_brain_annotation_dens # * voxel_volume\n",
    "    glia_dens_diff = abs(glia_dens_litt - glia_dens)\n",
    "    print(\"\\nAssertion on total glia densities (/mm^3)\")\n",
    "    print_range_bar(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance)\n",
    "    assertion_message = \"total glia densities out of literature range\"\n",
    "    z_score_assertion(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance, assertion_message)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Assertion on total inhibitory neuron densities\n",
    "    inhi_dens = np.sum(gad) / whole_brain_annotation_dens # * voxel_volume\n",
    "    diff_inhi = abs(inhibitory_neuron_dens_litt - inhi_dens)\n",
    "    print(\"\\nAssertion on total inhibitory neuron densities (/mm^3)\")\n",
    "    print(r\"/!\\ Tolerance set to default for neuron density\")\n",
    "    # print(\"/!\\ Default tolerance increased by a factor of 2.5\")\n",
    "    print_range_bar(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance)\n",
    "    # assert (diff_inhi <= inhibitory_neuron_dens_tolerance * 2.5) #, f\"diff_ini = \" + str(diff_inhi) + \"   |   tolerence = \" + str(inhibitory_neuron_dens_tolerance)\n",
    "    assertion_message = \"total inhibitory neuron densities out of literature range\"\n",
    "    z_score_assertion(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance, assertion_message)\n",
    "\n",
    "    print()\n",
    "\n",
    "    exci_inhib_sum_sum = np.sum(exci_inhib_sum) / whole_brain_annotation_dens # * voxel_volume\n",
    "    diff_exci_inhib_sum = abs(neuron_dens_litt - exci_inhib_sum_sum)\n",
    "    print(\"\\nAssertion on sum of inhibitory + excitatory neuron densities (/mm^3)\")\n",
    "    print_range_bar(exci_inhib_sum_sum, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance)\n",
    "    # print(\"/!\\ Data not available\")\n",
    "    assertion_message = \"sum of inhibitory + excitatory neuron densities out of literature range\"\n",
    "    z_score_assertion(exci_inhib_sum_sum, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance, assertion_message)\n",
    "\n",
    "\n",
    "    print(\"New Loop!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b54590-52cf-42a3-b07d-dd0d271fab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file in write mode\n",
    "with open(os.path.join(density_folder, \"output.txt\"), \"w\") as f:\n",
    "    # Save the current stdout so we can revert back later\n",
    "    original_stdout = sys.stdout\n",
    "    # Redirect stdout to the file\n",
    "    sys.stdout = f\n",
    "\n",
    "    for sc_factor in [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]: \n",
    "        try: del shuffled_combined_dataframes\n",
    "        except NameError: pass\n",
    "        \n",
    "        file = os.path.join(f'{root_folder}scaled_densities_new_{sc_factor}.pickle')\n",
    "        #Load region id volumes from volume_calc_from_template.ipynb\n",
    "        with open(file, 'rb') as pickle_file:\n",
    "            scaled_combined_result_dataframes = pickle.load(pickle_file)\n",
    "    \n",
    "        print()\n",
    "        print(\"Loaded pickle file.\", flush=True)\n",
    "        shuffled_combined_dataframes = create_combined_dataframe(scaled_combined_result_dataframes) #[celltypes]region\n",
    "    \n",
    "        def main():\n",
    "            # Define the parameters for each process\n",
    "            tasks = [\n",
    "                (neurontypes, \"scaled_total_neuron_densities\"),\n",
    "                (nonneurontypes, \"scaled_total_nonneuron_densities\"),\n",
    "                (exctypes, \"scaled_total_excitatory_densities\"),\n",
    "                (inhtypes, \"scaled_total_inhibitory_densities\"),\n",
    "                (astrotypes, \"scaled_total_astrotypes_densities\"),\n",
    "                (microglia, \"scaled_total_microglia_densities\"),\n",
    "                (oligos, \"scaled_total_oligocyte_densities\"),\n",
    "                (glia, \"scaled_total_glia_densities\"),\n",
    "                (exci_inhib_sum, \"scaled_total_excinh_densities\"),\n",
    "                (celltypes, \"scaled_total_celltypes_densities\"),\n",
    "            ]    \n",
    "            # Create a multiprocessing Pool\n",
    "            with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "                results = pool.starmap(process_type, tasks)\n",
    "            \n",
    "            # Sequentially write the .nrrd files to avoid concurrent writes\n",
    "            for result, file_name in results:\n",
    "                print(file_name, flush=True)\n",
    "                nrrd.write(f\"{root_folder}{file_name}.nrrd\", result)\n",
    "    \n",
    "        main()    \n",
    "    \n",
    "        cell, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_celltypes_densities.nrrd\"))\n",
    "        neuron, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_neuron_densities.nrrd\"))\n",
    "        \n",
    "        exc, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_excitatory_densities.nrrd\"))\n",
    "        gad, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_inhibitory_densities.nrrd\"))\n",
    "        microglia, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_microglia_densities.nrrd\"))\n",
    "        nn, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_nonneuron_densities.nrrd\"))\n",
    "        olig, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_oligocyte_densities.nrrd\"))\n",
    "        astro, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_astrotypes_densities.nrrd\"))\n",
    "        glia, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_glia_densities.nrrd\"))\n",
    "        exci_inhib_sum, __ = nrrd.read(os.path.join(density_folder, \"scaled_total_excinh_densities.nrrd\"))\n",
    "    \n",
    "    \n",
    "        z = 500\n",
    "        plt.imshow(cell[z], cmap='coolwarm');\n",
    "        # Define the coordinates\n",
    "        x = 222\n",
    "        y = 155\n",
    "        \n",
    "        name = region_map.get(annotation[z,y,x], \"name\", with_ascendants=False)\n",
    "        \n",
    "        plt.scatter(x, y, color='yellow', label='Point', s=10)\n",
    "        # Add labels and title\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title(f'The id at this point is {annotation[z,y,x]}, {name}, the value is {cell[z,y,x]}')\n",
    "        # Show the plot\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        print(annotation[z,y,x], name, \"value\", cell[z,y,x],\"(\", z, y, x, \")\", flush=True)\n",
    "    \n",
    "        # Assertion on total cell densities\n",
    "        cell_dens = np.sum(cell) / whole_brain_annotation_dens# * voxel_volume\n",
    "        cell_dens_diff = abs(cell_dens_litt - cell_dens)\n",
    "        print(\"\\nAssertion on total cell densities (/mm^3)\", flush=True)\n",
    "        print_range_bar(cell_dens, cell_dens_litt - cell_dens_tolerance, cell_dens_litt + cell_dens_tolerance)\n",
    "        assertion_message = \"total cell densities out of literature range\"\n",
    "        z_score_assertion(cell_dens, cell_dens_litt - cell_dens_tolerance, cell_dens_litt + cell_dens_tolerance, assertion_message)\n",
    "    \n",
    "        print()\n",
    "    \n",
    "        # Assertion on total neuron densities\n",
    "        neuron_dens = np.sum(neuron) / whole_brain_annotation_dens # * voxel_volume\n",
    "        neuron_dens_diff = abs(neuron_dens_litt - neuron_dens)\n",
    "        print(\"\\nAssertion on total neuron densities (/mm^3)\", flush=True)\n",
    "        print_range_bar(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance)\n",
    "        assertion_message = \"total neuron densities out of literature range\"\n",
    "        z_score_assertion(neuron_dens, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance, assertion_message)\n",
    "    \n",
    "        print()\n",
    "    \n",
    "        # Assertion on total glia densities\n",
    "        glia_dens = np.sum(glia) / whole_brain_annotation_dens # * voxel_volume\n",
    "        glia_dens_diff = abs(glia_dens_litt - glia_dens)\n",
    "        print(\"\\nAssertion on total glia densities (/mm^3)\", flush=True)\n",
    "        print_range_bar(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance)\n",
    "        assertion_message = \"total glia densities out of literature range\"\n",
    "        z_score_assertion(glia_dens, glia_dens_litt - glia_dens_tolerance, glia_dens_litt + glia_dens_tolerance, assertion_message)\n",
    "    \n",
    "        print()\n",
    "    \n",
    "        # Assertion on total inhibitory neuron densities\n",
    "        inhi_dens = np.sum(gad) / whole_brain_annotation_dens # * voxel_volume\n",
    "        diff_inhi = abs(inhibitory_neuron_dens_litt - inhi_dens)\n",
    "        print(\"\\nAssertion on total inhibitory neuron densities (/mm^3)\", flush=True)\n",
    "        print(r\"/!\\ Tolerance set to default for neuron density\", flush=True)\n",
    "        # print(\"/!\\ Default tolerance increased by a factor of 2.5\")\n",
    "        print_range_bar(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance)\n",
    "        # assert (diff_inhi <= inhibitory_neuron_dens_tolerance * 2.5) #, f\"diff_ini = \" + str(diff_inhi) + \"   |   tolerence = \" + str(inhibitory_neuron_dens_tolerance)\n",
    "        assertion_message = \"total inhibitory neuron densities out of literature range\"\n",
    "        z_score_assertion(inhi_dens, inhibitory_neuron_dens_litt - inhibitory_neuron_dens_tolerance, inhibitory_neuron_dens_litt + inhibitory_neuron_dens_tolerance, assertion_message)\n",
    "    \n",
    "        print()\n",
    "    \n",
    "        exci_inhib_sum_sum = np.sum(exci_inhib_sum) / whole_brain_annotation_dens # * voxel_volume\n",
    "        diff_exci_inhib_sum = abs(neuron_dens_litt - exci_inhib_sum_sum)\n",
    "        print(\"\\nAssertion on sum of inhibitory + excitatory neuron densities (/mm^3)\", flush=True)\n",
    "        print_range_bar(exci_inhib_sum_sum, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance)\n",
    "        # print(\"/!\\ Data not available\")\n",
    "        assertion_message = \"sum of inhibitory + excitatory neuron densities out of literature range\"\n",
    "        z_score_assertion(exci_inhib_sum_sum, neuron_dens_litt - neuron_dens_tolerance, neuron_dens_litt + neuron_dens_tolerance, assertion_message)\n",
    "    \n",
    "        print(\"New Loop!\", flush=True)\n",
    "        \n",
    "    # Reset stdout to its original state\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edbb3d-fcb6-4dca-ace4-3a38801806dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088be98b-5def-4948-af1c-1e948538d1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2ecb0-f69c-4ec3-9f59-a137cb93ea64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57d8a9-fe85-4d79-ba02-8f72d9d43586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99913f90-6b53-4f0d-af27-1b8dcb8ef9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
