{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25056fb5-06e3-43c0-acd3-e8b3bc623b7c",
   "metadata": {},
   "source": [
    "density1_estimation_all_region_XXslices_average_reconstcords_voxvol\n",
    "- density1: official pipeline step 1\n",
    "- estimation: we only have access to a subset of the cells\n",
    "- all_region: whole mouse brain (for now: except 3 problematic regions)\n",
    "- XX slices: MERFISH slices only 53\n",
    "- average: we don't split sides, but we combine slices for average if a region is crossed by multiple slices (not here, in density2)\n",
    "- reconstcords: we use the reconstructed coordinates from Allen (and not CCF or RAW coords)\n",
    "- voxvol: we estimate the area/volume using voxelised crossection between slice and CCFv3 (done in density0 as cells df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12e4d22-ae27-4eb4-8d34-6d49f41b1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import requests\n",
    "from scipy.spatial import Delaunay\n",
    "import nrrd\n",
    "from voxcell import RegionMap\n",
    "#import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/notebooks/scripts/')\n",
    "\n",
    "from helper_functions import calculate_density_mm3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037cc6db-83ed-407b-8bee-8281e380feda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6162ae3f-9a8e-4911-9334-0219ff26f96f",
   "metadata": {},
   "source": [
    "def plot_section(xx=None, yy=None, ax=None, cc=None, val=None, pcmap=None, \n",
    "                 overlay=None, extent=None, bcmap=plt.cm.Greys_r, alpha=1.0,\n",
    "                 fig_width=6, fig_height=6):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(fig_width, fig_height)\n",
    "\n",
    "    if xx is not None and yy is not None and pcmap is not None:\n",
    "        ax.scatter(xx, yy, s=20, c=val, marker='.', cmap=pcmap)\n",
    "    elif xx is not None and yy is not None and cc is not None:\n",
    "        ax.scatter(xx, yy, s=20, color=cc, marker='.', zorder=1)\n",
    "        \n",
    "    if overlay is not None and extent is not None and bcmap is not None:\n",
    "        ax.imshow(overlay, cmap=bcmap, extent=extent, alpha=alpha, zorder=2)\n",
    "        \n",
    "    ax.set_ylim(11, 0)\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.axis('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def plot_and_save_delaunay(points, tri, title, save_path):\n",
    "    # Plot the Delaunay triangulation\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.plot_trisurf(points[:, 0], points[:, 1], points[:, 2], triangles=tri.simplices, alpha=0.2)\n",
    "\n",
    "    # Optionally, you can also plot the original points\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], c='r', marker='o')\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    # Set the title\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Save the image instead of showing it\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the figure to free up resources\n",
    "\n",
    "# def calculate_density(cells_in_region, volume_single_cube_mm3, selected_region):\n",
    "#     '''Read nr of voxels and nr of cells in a region, calculate density in c/mm3, store as df'''\n",
    "#     data = []\n",
    "#     voxel_vol = np.sum(cells_in_region['voxel_vol'].unique())\n",
    "#     for cluster, count in cells_in_region['cluster'].value_counts().items():\n",
    "#         count_per_mm3_vol = count / (voxel_vol * volume_single_cube_mm3)\n",
    "#         data.append({'cluster': cluster, 'density_mm3': count_per_mm3_vol})\n",
    "\n",
    "#     result_df = pd.DataFrame(data)\n",
    "#     result_df.index.name = selected_region\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "def calculate_density_mm3(section, volume_single_cube_mm3, selected_region):\n",
    "    \"\"\"\n",
    "    Calculate density (count per mm3_vol) for each cluster in the given DataFrame section.\n",
    "\n",
    "    Parameters:\n",
    "        section (DataFrame): DataFrame containing 'cluster' and 'voxel_vol' columns.\n",
    "        volume_single_cube_mm3 (float): Volume of a single voxel in cubic millimeters.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing 'cluster' and 'density_mm3' columns.\n",
    "    \"\"\"\n",
    "    # Calculate density (count per voxel_vol) for each cluster\n",
    "    data = []\n",
    "    for cluster, count in section['cluster'].value_counts().items():\n",
    "        #Extract all slices voxel count once and take the sum since cell count is summed too\n",
    "        voxel_vol = section[['brain_section_label', 'voxel_vol']].drop_duplicates()['voxel_vol'].sum()\n",
    "        count_per_mm3_vol = count / (voxel_vol * volume_single_cube_mm3)\n",
    "        data.append({'cluster': cluster, 'density_mm3': count_per_mm3_vol})\n",
    "\n",
    "    # Create DataFrame from the calculated data\n",
    "    result_df = pd.DataFrame(data)\n",
    "    result_df.index.name = selected_region #Store real cell type name in every df\n",
    "    #result_df.set_index('cluster', inplace=True)\n",
    "    return result_df    \n",
    "\n",
    "\n",
    "def calculate_total_area(tri, points):\n",
    "    \"\"\"\n",
    "    Calculate the total area of a Delaunay triangulation.\n",
    "\n",
    "    Parameters:\n",
    "    - tri: Delaunay triangulation object\n",
    "    - points: 2D array, input points for triangulation\n",
    "\n",
    "    Returns:\n",
    "    - total_area: float, total area of the Delaunay triangulation\n",
    "    \"\"\"\n",
    "    total_area = 0.0\n",
    "\n",
    "    for simplex in tri.simplices:\n",
    "        # Extract the vertices of the triangle\n",
    "        triangle_vertices = points[simplex]\n",
    "\n",
    "        # Calculate two vectors representing sides of the triangle\n",
    "        AB = triangle_vertices[1] - triangle_vertices[0]\n",
    "        AC = triangle_vertices[2] - triangle_vertices[0]\n",
    "\n",
    "        # Calculate the cross product of AB and AC\n",
    "        cross_product = np.cross(AB, AC)\n",
    "\n",
    "        # Calculate the area of the triangle (half the magnitude of the cross product)\n",
    "        triangle_area = 0.5 * np.linalg.norm(cross_product)\n",
    "\n",
    "        # Sum up the areas\n",
    "        total_area += triangle_area\n",
    "\n",
    "    return total_area\n",
    "\n",
    "def delaunay_triangulation_with_perturbation(points, perturbation=1e-6):\n",
    "    \"\"\"\n",
    "    Perform Delaunay triangulation with perturbation to avoid coplanarity issues.\n",
    "\n",
    "    Parameters:\n",
    "    - points: 2D array of shape (n, 3) representing the coordinates of points.\n",
    "    - perturbation: Small value added to each coordinate to avoid coplanarity.\n",
    "\n",
    "    Returns:\n",
    "    - perturbed_points: 2D array of shape (n, 3) representing the perturbed points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the original points to avoid modifying the input array\n",
    "    perturbed_points = points.copy()\n",
    "\n",
    "    # Add a small perturbation to avoid coplanarity\n",
    "    perturbed_points += np.random.uniform(-perturbation, perturbation, size=perturbed_points.shape)\n",
    "\n",
    "    return perturbed_points\n",
    "\n",
    "def get_region_id(substructure):\n",
    "    region_id = int(substructure_info[substructure_info['parcellation_term_acronym'] == substructure]['parcellation_label'].values[0].split('-')[-1])\n",
    "    return region_id\n",
    "\n",
    "\n",
    "def process_slide_side(section, half, side, select_section, selected_region, download_base):\n",
    "               \n",
    "    # Filter DataFrame based on x_reconstructed values\n",
    "    subsection = section[section['x_reconstructed'] <= half] if side == \"right_side\" else section[section['x_reconstructed'] > half]\n",
    "    print(f\"Number of cells on the {side} of slide {select_section} of {selected_region} region is {subsection.shape[0]}.\", file=log_file)\n",
    "    \n",
    "    if section.shape[0] > 4: #requires a minimum of 5 points to construct the initial simplex        \n",
    "        # Convert DataFrame to NumPy array\n",
    "        points = subsection[['x_reconstructed', 'y_reconstructed', 'z_reconstructed']].values\n",
    "\n",
    "        # Perform Delaunay triangulation and save image\n",
    "        perturbed_points = delaunay_triangulation_with_perturbation(points)\n",
    "        tri = Delaunay(perturbed_points)\n",
    "        title = f\"{selected_region}_SLIDE{np.round(select_section, 2)}_{side}\"\n",
    "        #cluster_as_folder = selected_region.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"\", \" \":  r\"\", \",\": r\"\"}))\n",
    "        save_path = f\"{download_base}results/density_calculations_1/img/{cluster_as_folder}_{np.round(select_section, 2)}_dots_{side}.png\"\n",
    "        plot_and_save_delaunay(points, tri, title, save_path)\n",
    "\n",
    "        # Calculate total area\n",
    "        total_area = calculate_total_area(tri, points)\n",
    "        print(f\"Total Area of {selected_region}_SLIDE{np.round(select_section, 2)}_{side}:, {total_area} mm2.\", file=log_file)\n",
    "\n",
    "        # Estimate density: calculate volume of a 10 micron slice (volume = length × width × height)\n",
    "        densities = subsection['cluster'].value_counts() / (total_area * 0.01)\n",
    "        df_densities = pd.DataFrame({'cluster': densities.index, 'density': densities.values})\n",
    "    else:\n",
    "        print(f\"{selected_region}, {select_section}: {side} side and has not enough cells {subsection.shape[0]}, densities on this side will be 0.\", file=log_file)\n",
    "        densities = section['cluster'].value_counts() * 0 #Since the area is too small, densities will be 0.\n",
    "        df_densities = pd.DataFrame({'cluster': densities.index, 'density': densities.values})\n",
    "    \n",
    "    return df_densities\n",
    "\n",
    "def process_slide(section, select_section, selected_region, download_base):\n",
    "    print(f\"Number of cells on one-sided slide {select_section} of {selected_region} region is {section.shape[0]}.\", file=log_file)          \n",
    "\n",
    "    # Convert DataFrame to NumPy array\n",
    "    points = section[['x_reconstructed', 'y_reconstructed', 'z_reconstructed']].values\n",
    "\n",
    "    # Perform Delaunay triangulation and save image\n",
    "    perturbed_points = delaunay_triangulation_with_perturbation(points)\n",
    "    tri = Delaunay(perturbed_points)\n",
    "    title = f\"{selected_region}_SLIDE{np.round(select_section, 2)}_one-sided\"\n",
    "    #cluster_as_folder = selected_region.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"\", \" \":  r\"\", \",\": r\"\"}))\n",
    "    save_path = f\"{download_base}results/density_calculations_1/img/{cluster_as_folder}_{np.round(select_section, 2)}_dots_one-sided.png\"\n",
    "    plot_and_save_delaunay(points, tri, title, save_path)\n",
    "\n",
    "    # Calculate total area\n",
    "    total_area = calculate_total_area(tri, points)\n",
    "    print(f\"Total Area of {selected_region}_SLIDE{np.round(select_section, 2)}_one-sided:, {total_area} mm2.\", file=log_file)\n",
    "\n",
    "    # Estimate density: calculate volume of a 10 micron slice (volume = length × width × height)\n",
    "    densities = section['cluster'].value_counts() / (total_area * 0.01)\n",
    "    df_densities = pd.DataFrame({'cluster': densities.index, 'density': densities.values})\n",
    "\n",
    "    return df_densities\n",
    "\n",
    "def calculate_intersection_vol(CCFv3_0, region_id, z_index, intersection_angle_deg):\n",
    "    \n",
    "    CCFv3_0_copy = np.copy(CCFv3_0).astype(float)\n",
    "    \n",
    "    # Convert angle to radians\n",
    "    intersection_angle_rad = np.radians(intersection_angle_deg)\n",
    "    \n",
    "    # Define dimensions of the volume\n",
    "    volume_shape = CCFv3_0_copy.shape\n",
    "    array2D_width, array2D_height = volume_shape[2], volume_shape[1]\n",
    "\n",
    "    # Create meshgrid for x and y coordinates\n",
    "    x_coordinates, y_coordinates = np.meshgrid(np.arange(array2D_width), np.arange(array2D_height))\n",
    "\n",
    "    # Calculate y coordinate of the line at each x coordinate based on intersection angle\n",
    "    y_line = np.tan(intersection_angle_rad) * np.arange(array2D_width)\n",
    "    y_line_int_clipped = np.clip(np.round(y_line).astype(int), 0, array2D_height - 1)\n",
    "\n",
    "    # Create boolean mask to identify voxels in intersection region\n",
    "    mask = (y_coordinates <= y_line_int_clipped)\n",
    "\n",
    "    # Multiply voxels in intersection region by 2\n",
    "    CCFv3_0_copy[z_index, :, :][~mask] = 0\n",
    "    CCFv3_0_copy[z_index, :, :][mask] *= 2\n",
    "\n",
    "    # Count number of voxels affected by the plane\n",
    "    intersection_vol_in_voxels = np.count_nonzero(CCFv3_0_copy[z_index, :, :] == region_id*2)\n",
    "\n",
    "    return intersection_vol_in_voxels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7a485-812a-455d-b24e-29027e2d65e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11b750e-fcf5-48f3-8454-0ce5c41042d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All MERFISH cells: 3739961\n",
      "Filtered MERFISH cells: 3791571\n",
      "Columns dropped to 16\n",
      "CPU times: user 52.1 s, sys: 4.31 s, total: 56.4 s\n",
      "Wall time: 57.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "version = '20231215'\n",
    "download_base = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/'\n",
    "\n",
    "use_local_cache = False\n",
    "manifest_path = 'releases/%s/manifest.json' % version\n",
    "\n",
    "if not use_local_cache :\n",
    "    url = 'https://allen-brain-cell-atlas.s3-us-west-2.amazonaws.com/' + manifest_path\n",
    "    manifest = json.loads(requests.get(url).text)\n",
    "else :\n",
    "    file = os.path.join(download_base,manifest_path)\n",
    "    with open(file,'rb') as f:\n",
    "        manifest = json.load(f)\n",
    "        \n",
    "view_directory = os.path.join( download_base, \n",
    "                               manifest['directory_listing']['MERFISH-C57BL6J-638850-CCF']['directories']['metadata']['relative_path'], \n",
    "                              'views')\n",
    "\n",
    "file = os.path.join( view_directory, 'cell_metadata_with_parcellation_annotation.csv')\n",
    "\n",
    "#Load all MERFISH spatial data into 1 dataframe\n",
    "cell_joined = pd.read_csv(file)\n",
    "cell_joined.set_index('cell_label',inplace=True)\n",
    "print(f\"All MERFISH cells: {cell_joined.shape[0]}\")\n",
    "\n",
    "#Load voxelised region volume for each cell (note that this has less cells) \n",
    "view_directory = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/nmi_scores/'\n",
    "file = os.path.join( view_directory, 'cells_in_respective_volumes.csv')\n",
    "cells = pd.read_csv(file)\n",
    "cells.set_index('cell_label',inplace=True)\n",
    "\n",
    "# Combine the two dfs by removing 15 problematic cells and adding relevant columns\n",
    "# Identify common rows based on a unique identifier column\n",
    "common_rows = cells.index.intersection(cell_joined.index)\n",
    "# Filter cells_joined to retain only common rows\n",
    "cell_joined_filtered = cell_joined.loc[common_rows]\n",
    "# Specify the columns to add to cells_joined\n",
    "columns_to_add = ['template_nr', 'parcellation_substructure', 'parcellation_term_name', 'region_id', 'voxel_vol', ]\n",
    "# Merge cells_joined with the selected columns from cells based on the index\n",
    "cell_joined = cell_joined_filtered.merge(cells[columns_to_add], left_index=True, right_index=True, how='left')\n",
    "print(f\"Filtered MERFISH cells: {cell_joined.shape[0]}\")\n",
    "\n",
    "#We can make the main df lighter for the loop\n",
    "columns_to_drop = ['cluster_alias', 'average_correlation_score', 'feature_matrix_label', \n",
    "                   'donor_label', 'donor_genotype', 'donor_sex', 'neurotransmitter_color', \n",
    "                   'class_color', 'subclass_color', 'supertype_color', 'cluster_color', \n",
    "                   'parcellation_organ', 'parcellation_category', 'parcellation_division',\n",
    "                   'parcellation_structure', 'parcellation_organ_color', \n",
    "                   'parcellation_category_color', 'parcellation_division_color', \n",
    "                   'parcellation_structure_color', 'parcellation_substructure_color', \n",
    "                   'class', 'subclass', 'supertype', 'x_ccf', 'y_ccf', 'z_ccf', ]\n",
    "cell_joined.drop(columns=columns_to_drop, inplace=True)\n",
    "print(f\"Columns dropped to {cell_joined.shape[1]}\")\n",
    "\n",
    "#Load all parcellation data with extended info instead\n",
    "file = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/metadata/parcellation_to_parcellation_term_membership_extend.csv'\n",
    "parcellation_annotation = pd.read_csv(file)\n",
    "\n",
    "#This describes all indexes in the slices, but also on every level: organ category division structure substructure\n",
    "parcellation_indexes = list(np.unique(cell_joined['parcellation_index']))\n",
    "description_of_all_indexes = parcellation_annotation[parcellation_annotation['parcellation_index'].isin(parcellation_indexes)]\n",
    "\n",
    "#Create smaller dataframes\n",
    "substructure_info = description_of_all_indexes[description_of_all_indexes['parcellation_term_set_name'] == 'substructure'] \n",
    "\n",
    "hierarchy_json = '/gpfs/bbp.cscs.ch/data/project/proj84/atlas_pipeline_runs/2024-05-15T22:44:26+02:00/hierarchy_ccfv3_l23split_barrelsplit.json'\n",
    "region_map = RegionMap.load_json(hierarchy_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60167513-df66-4eb9-a88d-aef564e36cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame contains NaN values.\n",
      "Columns with NaN values: ['neurotransmitter']\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFrame contains any NaN values\n",
    "has_nan = cell_joined.isna().any().any()\n",
    "#has_nan = cell_joined['voxel_vol'].isna().any()\n",
    "\n",
    "if has_nan:\n",
    "    print(\"The DataFrame contains NaN values.\")\n",
    "    \n",
    "    # Get the columns with NaN values\n",
    "    columns_with_nan = cell_joined.columns[cell_joined.isna().any()].tolist()\n",
    "    print(\"Columns with NaN values:\", columns_with_nan)\n",
    "    \n",
    "else:\n",
    "    print(\"The DataFrame does not contain any NaN values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044d3f7-a0ad-4984-bc1f-8560fdff20f2",
   "metadata": {},
   "source": [
    "parcellation_substructure _x and _y come from the 2 dfs, they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fc2a21-b352-4678-a675-80d4bc565e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted directories: /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/csv /gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/log\n",
      "Directory csv and log and nrrd has been recreated or exist.\n"
     ]
    }
   ],
   "source": [
    "output_dir_csv = f\"{download_base}results/density_calculations/csv\"\n",
    "output_dir_log = f\"{download_base}results/density_calculations/log\"\n",
    "output_dir_nrrd = f\"{download_base}results/density_calculations/nrrd\"\n",
    "\n",
    "# Confirm the directory path before deletion\n",
    "if \"csv\" in output_dir_csv and os.path.exists(output_dir_csv):\n",
    "    shutil.rmtree(output_dir_csv)\n",
    "    shutil.rmtree(output_dir_log)\n",
    "    print(f\"Deleted directories: {output_dir_csv} {output_dir_log}\")\n",
    "else:\n",
    "    print(\"Directory path is not correct or does not exist.\")\n",
    "\n",
    "os.makedirs(output_dir_csv, exist_ok=True)\n",
    "os.makedirs(output_dir_log, exist_ok=True)\n",
    "os.makedirs(output_dir_nrrd, exist_ok=True)\n",
    "\n",
    "print(f\"Directory {output_dir_csv[99:]} and {output_dir_log[99:]} and {output_dir_nrrd[99:]} has been recreated or exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7deb2b2-19af-4d68-ba43-a5183f5e9863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe27b-036d-4427-a1ce-6b7f0904e927",
   "metadata": {},
   "source": [
    "- The next loop will open a file for logging. \n",
    "- It will loop through all 677 region and filter the cell dataframe (all 53 slices) based on these regional info. It will loop and print all extra substructures (leaf regions in the Cerebellum and 1 in the OLF) we added to the metadata. \n",
    "- It will create a csv files with densities of each cell type in a region. Filename: region name + density + two_sides (meaning if the code does not need to take into consideration whether two cells exist in two patches (i.e. the region exist on both hemispheres) or one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20b6056-23ea-4973-8e43-78ab906d3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.9 s, sys: 7.89 ms, total: 2.91 s\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_section_count = len(np.unique(cell_joined['brain_section_label']))\n",
    "# edge_length_mm = 10 * 0.001 # edge length of a 10 um3 voxel in mm\n",
    "# volume_single_cube_mm3 = edge_length_mm ** 3 #vol of a voxel in mm3\n",
    "volume_single_cube_mm3 = 1e-06\n",
    "\n",
    "# Specify the full or relative path to the log file\n",
    "root_folder = f\"{download_base}results/density_calculations/\"\n",
    "log_file_path = f\"{root_folder}/log/print_log_density1_part.txt\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3decb198-f14a-4eff-a810-c0885e55ec83",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "num_loops = 0\n",
    "\n",
    "for selected_region in substructure_info['parcellation_term_acronym'][0:]:\n",
    "    cells_in_region = cell_joined[cell_joined['parcellation_substructure_x'] == selected_region]\n",
    "    coronal_slice_positions = np.unique(cells_in_region['z_reconstructed'])\n",
    "    \n",
    "    # Add the number of coronal slice positions for the current selected region to num_loops\n",
    "    num_loops += len(coronal_slice_positions)\n",
    "    if selected_region == 'scp':\n",
    "        print(\"exists\")\n",
    "        break\n",
    "\n",
    "print(\"Total number of loops:\", num_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c2f3e2-989e-4e12-b5db-5765ca92a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOBglomerularlayer has an empty df 0 sections and 0 cells found..\n",
      "CENT2granularlayer has an empty df 0 sections and 0 cells found..\n",
      "CENT2purkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "CENT2molecularlayer has an empty df 0 sections and 0 cells found..\n",
      "CENT3granularlayer has an empty df 0 sections and 0 cells found..\n",
      "CENT3purkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "CENT3molecularlayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr1granularlayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr1purkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr1molecularlayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr2granularlayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr2purkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "ANcr2molecularlayer has an empty df 0 sections and 0 cells found..\n",
      "LINGgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "LINGpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "LINGmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "DECgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "DECpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "DECmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "FOTUgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "FOTUpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "FOTUmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "PYRgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "PYRpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "PYRmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "UVUgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "UVUpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "UVUmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "NODgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "NODpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "NODmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "SIMgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "SIMpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "SIMmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "PRMgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "PRMpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "PRMmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "PFLgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "PFLpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "PFLmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "FLgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "FLpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "FLmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "COPYgranularlayer has an empty df 0 sections and 0 cells found..\n",
      "COPYpurkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "COPYmolecularlayer has an empty df 0 sections and 0 cells found..\n",
      "CUL45granularlayer has an empty df 0 sections and 0 cells found..\n",
      "CUL45purkinjelayer has an empty df 0 sections and 0 cells found..\n",
      "CUL45molecularlayer has an empty df 0 sections and 0 cells found..\n",
      "CPU times: user 11min 57s, sys: 190 ms, total: 11min 58s\n",
      "Wall time: 11min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    for selected_region in substructure_info['parcellation_term_acronym'][0:]: #Selecting cell type\n",
    "        \n",
    "        #print(selected_region) \n",
    "        #The base df is in the next line: all cells belonging to one region across many sections\n",
    "        cells_in_region = cell_joined[cell_joined['parcellation_substructure_x'] == selected_region]\n",
    "        nr_of_cells = cell_joined[cell_joined['parcellation_substructure_x'] == selected_region].shape[0]\n",
    "        sections = len(np.unique(cells_in_region['brain_section_label']))       \n",
    "        ctypes_in_region = len(np.unique(cells_in_region['cluster']))\n",
    "        #region_id = int(substructure_info[substructure_info['parcellation_term_acronym'] == 'VISpl6b']['parcellation_label'].values[0].split('-')[-1])\n",
    "        #region_id = set(cells_in_region['region_id'].values)\n",
    "        log_file.flush()  # Flush the buffer to ensure immediate write\n",
    "        print(\"\", file=log_file)\n",
    "        print(f'There are {nr_of_cells} cells found in the {selected_region} which belong into {ctypes_in_region} cell types', file=log_file)\n",
    "        print(f'There are {sections} sections which intersect the {selected_region} out of {all_section_count}.', file=log_file)\n",
    "        #coronal_slice_positions = np.unique(cells_in_region['z_reconstructed'])\n",
    "        cluster_as_filename = selected_region.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"\", \" \":  r\"\", \",\": r\"\"}))\n",
    "        result_df = calculate_density_mm3(cells_in_region, volume_single_cube_mm3, selected_region)\n",
    "        if not result_df.empty:\n",
    "            result_df.to_csv(f\"{root_folder}csv/{cluster_as_filename}_density_two_sides.csv\", index=True)\n",
    "        else:\n",
    "            print(f\"{selected_region} has an empty df {sections} sections and {nr_of_cells} cells found..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c282eb-b304-4c25-90d1-643d66662192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/notebooks/scripts/')\n",
    "\n",
    "from helper_functions import get_all_filenames, get_csv_filenames, extract_prefix_from_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0fa26c0-63fb-4e57-a989-6cc0bd5521aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all regional density data\n",
    "folder_path = f\"{root_folder}csv/\"\n",
    "filenames = get_all_filenames(folder_path)\n",
    "csv_filenames = get_csv_filenames(folder_path)\n",
    "prefixes = extract_prefix_from_filenames(csv_filenames)\n",
    "unique_prefixes = sorted(list(set(prefixes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3803eb-7d75-4213-be5c-4fc8aff019e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max leaf region length: 677\n",
    "len(csv_filenames)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05984168-c261-49d2-94bc-be600d31904a",
   "metadata": {
    "tags": []
   },
   "source": [
    "def read_and_test_csv_files(filenames, unique_prefixes, folder_path):\n",
    "    '''Test all files created above, whether they are empty and the columns are in order.'''\n",
    "    for prefix in unique_prefixes:\n",
    "        matching_files = [filename for filename in filenames if filename.startswith(prefix)]\n",
    "        if matching_files:\n",
    "            for filename in matching_files:\n",
    "                df = pd.read_csv(os.path.join(folder_path, filename), index_col='cluster')\n",
    "                if df.empty:\n",
    "                    print(f\"The DataFrame from {filename} is empty\")\n",
    "                    continue  # Skip further checks if the DataFrame is empty\n",
    "                \n",
    "                # Check if 'cluster' is the index and 'density_mm3' is a column\n",
    "                if df.index.name != 'cluster' or 'density_mm3' not in df.columns:\n",
    "                    print(f\"The DataFrame from {filename} has incorrect index or missing 'density_mm3' column\")\n",
    "                    continue  # Skip further checks if the index or column is incorrect\n",
    "               \n",
    "                # Check data types of 'cluster' and 'density_mm3' columns\n",
    "                if df.index.dtype != object or not pd.api.types.is_numeric_dtype(df['density_mm3']):\n",
    "                    print(f\"The DataFrame from {filename} has incorrect data types\")\n",
    "                    continue  # Skip further checks if data types are incorrect\n",
    "                \n",
    "                # If all checks pass, store the DataFrame\n",
    "                #result_dataframes[filename] = df\n",
    "                \n",
    "    print(\"If there are no print messages before this, all DataFrames / csv files contains cells and densities\")\n",
    "                \n",
    "\n",
    "read_and_test_csv_files(csv_filenames, unique_prefixes, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cfbebfa-f6f3-4c04-8925-bd2dbe393e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If there are no print messages before this, all DataFrames / csv files contain cells and densities.\n"
     ]
    }
   ],
   "source": [
    "def read_and_test_csv_files(filenames, unique_prefixes, folder_path):\n",
    "    '''Test all files created above, whether they are empty and the columns are in order.'''\n",
    "    for prefix in unique_prefixes:\n",
    "        matching_files = [filename for filename in filenames if filename.startswith(prefix)]\n",
    "        if matching_files:\n",
    "            for filename in matching_files:\n",
    "                try:\n",
    "                    # Read the CSV file\n",
    "                    df = pd.read_csv(os.path.join(folder_path, filename), index_col='cluster')\n",
    "                except StopIteration:\n",
    "                    print(f\"StopIteration encountered when reading {filename}. The file might be corrupted or incorrectly formatted.\")\n",
    "                    continue  # Skip to the next file\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filename}: {e}\")\n",
    "                    continue  # Skip further checks if reading failed\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"The DataFrame from {filename} is empty\")\n",
    "                    continue  # Skip further checks if the DataFrame is empty\n",
    "\n",
    "                # Check if 'cluster' is the index and 'density_mm3' is a column\n",
    "                if df.index.name != 'cluster' or 'density_mm3' not in df.columns:\n",
    "                    print(f\"The DataFrame from {filename} has incorrect index or missing 'density_mm3' column\")\n",
    "                    continue  # Skip further checks if the index or column is incorrect\n",
    "                \n",
    "                # Convert 'density_mm3' to numeric if it's not already\n",
    "                try:\n",
    "                    df['density_mm3'] = pd.to_numeric(df['density_mm3'], errors='raise')\n",
    "                except ValueError:\n",
    "                    print(f\"Cannot convert 'density_mm3' to numeric in {filename}. Data might be corrupted or invalid.\")\n",
    "                    continue  # Skip further checks if conversion fails\n",
    "\n",
    "                # Check data types of 'cluster' and 'density_mm3' columns\n",
    "                if df.index.dtype != object or not pd.api.types.is_numeric_dtype(df['density_mm3']):\n",
    "                    print(f\"The DataFrame from {filename} has incorrect data types\")\n",
    "                    continue  # Skip further checks if data types are incorrect\n",
    "\n",
    "                # If all checks pass, store the DataFrame\n",
    "                # result_dataframes[filename] = df\n",
    "\n",
    "    print(\"If there are no print messages before this, all DataFrames / csv files contain cells and densities.\")\n",
    "\n",
    "read_and_test_csv_files(csv_filenames, unique_prefixes, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568dc360-2c7f-4aa8-a321-a5bdc8221589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603941aa-a33c-43be-b37b-bf1c0f161973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test next step (density2 step): \n",
    "density_comp_nrrd_jobarray.sh + density_comp_nrrd_on_1node.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b5c266-7ac9-4917-8e50-dd9e3e9dcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/notebooks/scripts/')\n",
    "\n",
    "from helper_functions import get_all_filenames, get_csv_filenames, extract_prefix_from_filenames, get_nrrd_files, nrrd_from_df\n",
    "from helper_functions import read_and_concat_csv_files_new, combine_rows_and_calculate_average, create_combined_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e7a968-8e68-4840-bc15-815fba0d461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all csv files with densities\n",
    "root_folder = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/results/density_calculations/'\n",
    "folder_path = f'{root_folder}csv/'\n",
    "filenames = get_all_filenames(folder_path)\n",
    "csv_filenames = get_csv_filenames(folder_path)\n",
    "prefixes = extract_prefix_from_filenames(csv_filenames)\n",
    "unique_prefixes = sorted(list(set(prefixes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9496d0b-fbfc-4665-82d1-bd053a9a1f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Create a dict of df, each containing a cell type's occurence in all regions and its densities in all regions\n",
    "result_dataframes = read_and_concat_csv_files_new(csv_filenames, unique_prefixes, folder_path)\n",
    "combined_result_dataframes = combine_rows_and_calculate_average(result_dataframes)\n",
    "shuffled_combined_dataframes = create_combined_dataframe(combined_result_dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49e2f0-6307-4c1f-826d-7d60a8fcd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Get the set of all clusters present in the result_dataframes\n",
    "all_clusters = set()\n",
    "for df in result_dataframes.values():\n",
    "    all_clusters.update(df.index)\n",
    "\n",
    "cluster_ids = []\n",
    "for cluster in all_clusters: \n",
    "    match = re.search(r'\\d+', cluster)\n",
    "    if match:\n",
    "        # Extract the first 4 numbers\n",
    "        cluster = match.group()[:4]\n",
    "    else:\n",
    "        # If no numeric portion is found, keep the original string\n",
    "        cluster = cluster\n",
    "        print(f\"No numeric portion is found, keeping original name for: {cluster}\", flush=True)\n",
    "    \n",
    "    cluster_ids.append(cluster)\n",
    "     \n",
    "cluster_ids = sorted(cluster_ids)\n",
    "\n",
    "#region_list = list(combined_result_dataframes.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29be317-142b-408f-bb79-791ef9226a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This block is to get all region names for placement into CCFv3 space\n",
    "view_directory = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/metadata/MERFISH-C57BL6J-638850-CCF/20231215/views'\n",
    "file = os.path.join( view_directory, 'cell_metadata_with_parcellation_annotation.csv')\n",
    "cell_joined = pd.read_csv(file)\n",
    "cell_joined.set_index('cell_label',inplace=True)\n",
    "\n",
    "\n",
    "#Read CCFv3 annotation volume\n",
    "data_folder = \"/gpfs/bbp.cscs.ch/project/proj84/piluso/share/general/warped_augmented_CCFv3/\"\n",
    "CCFv3_0, _ = nrrd.read(f'{data_folder}annotation_25_2022_CCFv3_0.nrrd')\n",
    "# CCFv3_0, _ = nrrd.read(\"/gpfs/bbp.cscs.ch/data/project/proj84/atlas_pipeline_runs/2024-05-15T22:44:26+02:00/annotation_ccfv3_l23split_barrelsplit_validated.nrrd\"\n",
    "\n",
    "save_nrrd = f'{root_folder}nrrd/'\n",
    "# Specify the full or relative path to the log file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59ec3c2-137c-4e59-b349-e228a767798e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame for cluster '0589 OB Dopa-Gaba_1':\n",
      "0589_OB_Dopa_Gaba_1 is not created yet.\n",
      "ACB 2 (region name and its subregions)\n",
      "AIv1 1 (region name is a leaf region )\n",
      "AIv23 1 (region name is a leaf region )\n",
      "AOBgl 1 (region name is a leaf region )\n",
      "AOBmi 1 (region name is a leaf region )\n",
      "AON 2 (region name and its subregions)\n",
      "CP 2 (region name and its subregions)\n",
      "DP 2 (region name and its subregions)\n",
      "IG 2 (region name and its subregions)\n",
      "MOBgr 1 (region name is a leaf region )\n",
      "MOBipl 1 (region name is a leaf region )\n",
      "MOBmi 1 (region name is a leaf region )\n",
      "MOBopl 1 (region name is a leaf region )\n",
      "MOBunassigned 1 (region name is a leaf region )\n",
      "MOp6b 1 (region name is a leaf region )\n",
      "OLFunassigned 2 (region name and its subregions)\n",
      "ORBl1 1 (region name is a leaf region )\n",
      "ORBm1 1 (region name is a leaf region )\n",
      "ORBvl1 1 (region name is a leaf region )\n",
      "PIR 2 (region name and its subregions)\n",
      "SSpll6a 1 (region name is a leaf region )\n",
      "STRunassigned 2 (region name and its subregions)\n",
      "TTd 1 (region name is a leaf region )\n",
      "TTv 1 (region name is a leaf region )\n",
      "VISC1 1 (region name is a leaf region )\n",
      "VISp1 1 (region name is a leaf region )\n",
      "cing 1 (region name is a leaf region )\n",
      "fa 2 (region name and its subregions)\n",
      "fp 1 (region name is a leaf region )\n",
      "lot 1 (region name is a leaf region )\n",
      "onl 1 (region name is a leaf region )\n",
      "unassigned 5 (region name and its subregions)\n",
      "Saving file: 0589 OB Dopa-Gaba_1 as 0589_OB_Dopa_Gaba_1\n",
      "\n",
      "\n",
      "1\n",
      "Combined DataFrame for cluster '2291 VMH Fezf1 Glut_1':\n",
      "2291_VMH_Fezf1_Glut_1 is not created yet.\n",
      "HYunassigned 2 (region name and its subregions)\n",
      "PeF 2 (region name and its subregions)\n",
      "TU 2 (region name and its subregions)\n",
      "VMH 2 (region name and its subregions)\n",
      "Saving file: 2291 VMH Fezf1 Glut_1 as 2291_VMH_Fezf1_Glut_1\n",
      "\n",
      "\n",
      "2\n",
      "Combined DataFrame for cluster '0049 L6 IT CTX Glut_4':\n",
      "0049_L6_IT_CTX_Glut_4 is not created yet.\n",
      "ACAv23 1 (region name is a leaf region )\n",
      "ACAv5 1 (region name is a leaf region )\n",
      "ACAv6a 1 (region name is a leaf region )\n",
      "AId23 1 (region name is a leaf region )\n",
      "AId5 1 (region name is a leaf region )\n",
      "AId6a 1 (region name is a leaf region )\n",
      "AIp5 1 (region name is a leaf region )\n",
      "AIv5 1 (region name is a leaf region )\n",
      "AIv6a 1 (region name is a leaf region )\n",
      "APr 2 (region name and its subregions)\n",
      "AUDp6a 1 (region name is a leaf region )\n",
      "AUDv5 1 (region name is a leaf region )\n",
      "CLA 2 (region name and its subregions)\n",
      "DP 2 (region name and its subregions)\n",
      "ECT5 1 (region name is a leaf region )\n",
      "ECT6a 1 (region name is a leaf region )\n",
      "ENTl5 1 (region name is a leaf region )\n",
      "ENTl6a 1 (region name is a leaf region )\n",
      "ENTm6 1 (region name is a leaf region )\n",
      "GU5 1 (region name is a leaf region )\n",
      "GU6a 1 (region name is a leaf region )\n",
      "IG 2 (region name and its subregions)\n",
      "ILA23 1 (region name is a leaf region )\n",
      "ILA5 1 (region name is a leaf region )\n",
      "ILA6a 1 (region name is a leaf region )\n",
      "MOp6a 1 (region name is a leaf region )\n",
      "ORBl23 1 (region name is a leaf region )\n",
      "ORBm23 1 (region name is a leaf region )\n",
      "ORBm5 1 (region name is a leaf region )\n",
      "ORBm6a 1 (region name is a leaf region )\n",
      "ORBvl23 1 (region name is a leaf region )\n",
      "ORBvl5 1 (region name is a leaf region )\n",
      "PL5 1 (region name is a leaf region )\n",
      "PL6a 1 (region name is a leaf region )\n",
      "POST 2 (region name and its subregions)\n",
      "RSPagl5 1 (region name is a leaf region )\n",
      "RSPagl6a 1 (region name is a leaf region )\n",
      "RSPd5 1 (region name is a leaf region )\n",
      "RSPd6a 1 (region name is a leaf region )\n",
      "RSPv5 1 (region name is a leaf region )\n",
      "SSs5 1 (region name is a leaf region )\n",
      "SSs6a 1 (region name is a leaf region )\n",
      "TEa5 1 (region name is a leaf region )\n",
      "TEa6a 1 (region name is a leaf region )\n",
      "VISC5 1 (region name is a leaf region )\n",
      "VISC6a 1 (region name is a leaf region )\n",
      "VISam6a 1 (region name is a leaf region )\n",
      "VISpor5 1 (region name is a leaf region )\n",
      "VISpor6a 1 (region name is a leaf region )\n",
      "fa 2 (region name and its subregions)\n",
      "scwmunassigned 2 (region name and its subregions)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not created yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#code\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result_CCFv3_0_copy \u001b[38;5;241m=\u001b[39m \u001b[43mnrrd_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription_of_all_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCCFv3_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/notebooks/scripts/helper_functions.py:334\u001b[0m, in \u001b[0;36mnrrd_from_df\u001b[0;34m(df, description_of_all_indexes, CCFv3_0, region_map)\u001b[0m\n\u001b[1;32m    332\u001b[0m     density_value \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    333\u001b[0m     region_id \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 334\u001b[0m     CCFv3_0_copy[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCCFv3_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_id\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m density_value\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m#Create outside of the brain as np.nan\u001b[39;00m\n\u001b[1;32m    337\u001b[0m CCFv3_0_copy[np\u001b[38;5;241m.\u001b[39misin(CCFv3_0, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0\u001b[39m))] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/site-packages/numpy/lib/arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m       [ True, False]])\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(element)\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43min1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(element\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/data/project/proj84/csaba/git/cellden/lib/python3.12/site-packages/numpy/lib/arraysetops.py:618\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03mTest whether each element of a 1-D array is also present in a second array.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03marray([1, 5])\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Ravel both arrays, behavior for the first array could be different\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m ar1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m ar2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(ar2)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Ensure that iteration through object arrays yields size-1 arrays\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for cluster, df in shuffled_combined_dataframes.items():\n",
    "    print(f\"Combined DataFrame for cluster '{cluster}':\", flush=True)\n",
    "    f_name = cluster.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"_\", \" \":  r\"_\"}))\n",
    "    created_nrrds, nrrd_files_without_extension = get_nrrd_files(save_nrrd)\n",
    "    if f_name not in nrrd_files_without_extension:\n",
    "        print(f\"{f_name} is not created yet.\", flush=True)\n",
    "\n",
    "        #code\n",
    "        result_CCFv3_0_copy = nrrd_from_df(df, description_of_all_indexes, CCFv3_0, region_map)\n",
    "\n",
    "        print(f\"Saving file: {cluster} as {f_name}\" )\n",
    "        print(\"\\n\", flush=True)\n",
    "        nrrd.write(f\"{save_nrrd}{f_name}.nrrd\", result_CCFv3_0_copy)\n",
    "        #iterate      \n",
    "        count += 1\n",
    "        print(count, flush=True)\n",
    "        # if count == 1:\n",
    "        #     break\n",
    "\n",
    "    else:\n",
    "        print(f\"Exception occurred for cluster {cluster}\", flush=True)\n",
    "        # Handle the exception if needed\n",
    "        print(\"\\n\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a723f-d8d0-4e4d-8a0b-dcf190beeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrrd_files_without_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680ef76-612b-46cf-9c19-84c7696ce3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe15503c-c267-40f0-bf9c-4fd2dbf1ea24",
   "metadata": {},
   "source": [
    "## Test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aed73-26a8-471f-832a-a4841f488a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(shuffled_combined_dataframes.items())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e642586a-c021-416a-b58b-813591f1afb2",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fec53881-6671-480e-b523-6c32236cb786",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "shuffled_combined_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090643a-13d3-45b7-a799-6725402fae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster, df in shuffled_combined_dataframes.items():\n",
    "    if cluster == '5252 Ependymal NN_1':\n",
    "        print(f\"Combined DataFrame for cluster '{cluster}':\", flush=True)\n",
    "        f_name = cluster.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"_\", \" \":  r\"_\"}))\n",
    "        #created_nrrds, nrrd_files_without_extension = get_nrrd_files(save_nrrd)\n",
    "        all_ids_for_df = []\n",
    "        df_comb = pd.DataFrame()\n",
    "    \n",
    "        for regionname in df.index.values:\n",
    "            density = df.loc[regionname][0]        \n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c41953-ab39-4db2-964e-3090358cd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[regionname, 'density_mm3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba204c-1685-47a4-9343-4a254988beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02735d69-1dcd-4a79-9fa0-9750f0518ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "all_ids_for_df = []\n",
    "df_comb = pd.DataFrame()\n",
    "\n",
    "# for cluster, df in shuffled_combined_dataframes.items():\n",
    "#     #if cluster == '3890 MB-MY Tph2 Glut-Sero_2':\n",
    "#         print(f\"Combined DataFrame for cluster '{cluster}':\", flush=True)\n",
    "#         f_name = cluster.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"_\", \" \":  r\"_\"}))\n",
    "for regionname in df.index.values:\n",
    "        #if regionname == 'SSpbfd1':\n",
    "        #if regionname == 'CS':\n",
    "            #print(regionname)\n",
    "            density = df.loc[regionname][0]\n",
    "            #print(density)\n",
    "            #annotation_id_info = substructure_info[substructure_info['cluster_as_filename'] == regionname]\n",
    "            annotation_id_info = description_of_all_indexes[description_of_all_indexes['cluster_as_filename'] == regionname]\n",
    "            Annotation2020ids = list(annotation_id_info['label_numbers'].values) \n",
    "            \n",
    "            Annotation2020ids = [int(re.search(r'\\d+$', s).group()) for s in annotation_id_info['parcellation_label'].values]\n",
    "            #print(Annotation2020ids, annotation_id_info.shape)\n",
    "            df_sub = pd.DataFrame({'density': density}, index=Annotation2020ids)\n",
    "            df_comb = pd.concat([df_comb, df_sub])\n",
    "    \n",
    "            all_ids_for_df.append(Annotation2020ids)\n",
    "            if annotation_id_info.shape[0] == 0:\n",
    "                print(regionname, \" region was not found and the region's density will be 0!!!\", flush=True)\n",
    "            elif annotation_id_info.shape[0] == 1:\n",
    "                print(regionname, annotation_id_info.shape[0], \"(region name is a leaf region )\", flush=True)\n",
    "            else:\n",
    "                print(regionname, annotation_id_info.shape[0], \"(region name and its subregions)\", flush=True)\n",
    "all_ids_for_df = [value for sublist in all_ids_for_df for value in sublist]\n",
    "all_ids_for_df.append(0)\n",
    "\n",
    "outside = 0\n",
    "outsideid = [0]\n",
    "df_sub = pd.DataFrame({'density': outside}, index=outsideid)\n",
    "df_comb = pd.concat([df_comb, df_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b933de-7ce4-481d-a62e-988dd4294dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241dcc2-87a5-4bbb-b316-ff1403e45fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for cluster, df in shuffled_combined_dataframes.items():\n",
    "    if cluster == '0629 Vip Gaba_2':\n",
    "        \n",
    "        print(f\"Combined DataFrame for cluster '{cluster}':\", flush=True)\n",
    "        f_name = cluster.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"_\", \" \":  r\"_\"}))\n",
    "        #created_nrrds, nrrd_files_without_extension = get_nrrd_files(save_nrrd)\n",
    "        \n",
    "        all_ids_for_df = []\n",
    "        df_comb = pd.DataFrame()\n",
    "        \n",
    "        for regionname in df.index.values:\n",
    "            density = df.loc[regionname][0]\n",
    "            annotation_id_info = description_of_all_indexes[description_of_all_indexes['parcellation_term_acronym'] == regionname]\n",
    "            Annotation2020ids = [int(re.search(r'\\d+$', s).group()) for s in annotation_id_info['parcellation_label'].values]\n",
    "            df_sub = pd.DataFrame({'density': density}, index=Annotation2020ids)\n",
    "            df_comb = pd.concat([df_comb, df_sub])\n",
    "        \n",
    "            all_ids_for_df.append(Annotation2020ids)\n",
    "        \n",
    "            print(regionname, annotation_id_info.shape[0], \"(region name and its subregions)\", flush=True)\n",
    "        \n",
    "        all_ids_for_df = [value for sublist in all_ids_for_df for value in sublist]\n",
    "        all_ids_for_df.append(0)\n",
    "        \n",
    "        outside = 0\n",
    "        outsideid = [0]\n",
    "        df_sub = pd.DataFrame({'density': outside}, index=outsideid)\n",
    "        df_comb = pd.concat([df_comb, df_sub])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(f\"{f_name} is not created yet.\", flush=True)\n",
    "        \n",
    "        # #code\n",
    "        # result_CCFv3_0_copy = nrrd_from_df(df, description_of_all_indexes, CCFv3_0)\n",
    "        \n",
    "        # print(f\"Saving file: {cluster} as {f_name}\" )\n",
    "        # print(\"\\n\", flush=True)\n",
    "        #nrrd.write(f\"{save_nrrd}{f_name}.nrrd\", result_CCFv3_0_copy)\n",
    "        #iterate      \n",
    "        count += 1\n",
    "        print(count, flush=True)\n",
    "        if count == 1:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3101d4-bd0b-41cc-a334-8e6462b446d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster, f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64326de8-c570-4c08-801d-84f6ee7b8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_info['label_numbers'] = substructure_info['parcellation_label'].str.extract(r'AllenCCF-Annotation-2020-(\\d+)')\n",
    "substructure_info['label_numbers'] = substructure_info['label_numbers'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e673146-54c1-47aa-93f2-91ac9d573fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_info['cluster_as_filename'] = substructure_info['parcellation_term_acronym'].str.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"\", \" \":  r\"\", \",\": r\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8484e-7a48-4875-bda0-2c4f976ba345",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_annotation['label_numbers'] = parcellation_annotation['parcellation_label'].str.extract(r'AllenCCF-Annotation-2020-(\\d+)')\n",
    "parcellation_annotation['label_numbers'] = parcellation_annotation['label_numbers'].astype(int)\n",
    "parcellation_annotation['cluster_as_filename'] = parcellation_annotation['parcellation_term_acronym'].str.translate(str.maketrans({\"/\":  r\"\", \"-\":  r\"\", \" \":  r\"\", \",\": r\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cc6b6-51d0-40e5-b3fc-5b15a70d9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d396e8a-3fbd-484e-b801-7a16d43ae216",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/gpfs/bbp.cscs.ch/data/project/proj84/csaba/aibs_10x_mouse_wholebrain/metadata/parcellation_to_parcellation_term_membership_extend.csv'\n",
    "parcellation_annotation.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6140da-71b7-4481-be64-1d0310ae3527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a774b63-ad0d-463b-b868-6d205b98edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01dbc6-ba09-4a35-8fee-a4a80f670403",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_in_region[['z_section', 'template_nr', 'best_position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83208883-d99e-4a8a-b2e3-c63f432dbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCFv3_0_copy = np.copy(CCFv3_0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d1d8e-2ee3-4aab-9cf1-b8e18eade2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where True indicates the elements that match the target value\n",
    "mask = (CCFv3_0_copy == region_id)\n",
    "\n",
    "# Set all elements to 0 except the ones that match the target value\n",
    "CCFv3_0_copy[~mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8326de-c998-4079-ad60-016024c345a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(CCFv3_0_copy[z_index]);\n",
    "plt.title(f\"z_index is {z_index}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c4210-a5d7-411a-9952-13d896c46398",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(CCFv3_0_copy == 393)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eaae6-1993-4785-950e-3c5b4f61e143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd32f3-5738-4983-92eb-a38eeef0d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continuing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b4654-ffae-449e-b112-28b1d07abb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCFv3_0_copy = np.copy(CCFv3_0).astype(float)\n",
    "\n",
    "# Convert angle to radians\n",
    "intersection_angle_rad = np.radians(intersection_angle_deg)\n",
    "\n",
    "# Define dimensions of the volume\n",
    "volume_shape = CCFv3_0_copy.shape\n",
    "array2D_width, array2D_height = volume_shape[2], volume_shape[1]\n",
    "\n",
    "# Create meshgrid for x and y coordinates\n",
    "x_coordinates, y_coordinates = np.meshgrid(np.arange(array2D_width), np.arange(array2D_height))\n",
    "\n",
    "# Calculate y coordinate of the line at each x coordinate based on intersection angle\n",
    "y_line = np.tan(intersection_angle_rad) * np.arange(array2D_width)\n",
    "y_line_int_clipped = np.clip(np.round(y_line).astype(int), 0, array2D_height - 1)\n",
    "\n",
    "# Create boolean mask to identify voxels in intersection region\n",
    "mask = (y_coordinates <= y_line_int_clipped)\n",
    "\n",
    "# Multiply voxels in intersection region by 2\n",
    "CCFv3_0_copy[z_index, :, :][~mask] = 0\n",
    "CCFv3_0_copy[z_index, :, :][mask] *= 2\n",
    "\n",
    "# Count number of voxels affected by the plane\n",
    "intersection_vol_in_voxels = np.count_nonzero(CCFv3_0_copy[z_index, :, :] == region_id*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018ec3a-b198-493f-8443-10ccb52ef0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441350ab-be55-43a1-beec-f2af505f019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_coordinates);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb3fbd-083f-4c77-91eb-9df93e444f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(CCFv3_0_copy[402], cmap='coolwarm');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
